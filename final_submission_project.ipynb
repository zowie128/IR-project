{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:44:01.318647Z",
     "start_time": "2024-03-30T07:43:58.303439Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyterrier as pt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "# Ensure NLTK data is downloaded only once\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Define file paths\n",
    "original_queries_path = 'original_queries.json'\n",
    "rewritten_queries_path = 'rewritten_queries.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Functions to save and load data\n",
    "def save_data_to_file(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def load_data_from_file(file_path):\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    return None\n",
    "\n",
    "def remove_redundancy(preprocessed_query):\n",
    "    tokens = preprocessed_query.split()\n",
    "    seen = set()\n",
    "    unique_tokens = [t for t in tokens if not (t in seen or seen.add(t))]\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "# Load spaCy English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extend stop words list with common but low-information words\n",
    "extended_stop_words = {\"define\", \"meaning\", \"example\", \"describe\", \"use\", \"refer\", \"relate\", \"involve\", \"include\", \"give\", \"take\", \"make\", \"see\", \"want\", \"get\", \"say\", \"ask\", \"tell\", \"be\", \"know\", \"do\", \"have\", \"would\", \"should\", \"could\", \"about\"}\n",
    "for word in extended_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "# Customize token extension to flag important tokens to keep\n",
    "Token.set_extension(\"is_important\", default=False, force=True)\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Preprocess a single query using spaCy for tokenization, lemmatization, and stop word removal,\n",
    "    aiming for greater conciseness.\n",
    "    \"\"\"\n",
    "    # Clean up the query by removing unwanted characters\n",
    "    query = re.sub(r'\\n+', ' ', query)  # Replace one or more newlines with a single space\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(query)\n",
    "\n",
    "    # Identify important tokens to preserve\n",
    "    for ent in doc.ents:\n",
    "        for token in ent:\n",
    "            token._.is_important = True\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"PROPN\", \"NOUN\", \"VERB\"}:\n",
    "            token._.is_important = True\n",
    "\n",
    "    # Condense the query by keeping important tokens and removing less important ones\n",
    "    tokens = [token.lemma_.lower() for token in doc if (token._.is_important or token.text.lower() in extended_stop_words) and not token.is_stop and token.pos_ != \"PUNCT\"]\n",
    "\n",
    "    # Reconstruct the query\n",
    "    preprocessed_query = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_query\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Performs common cleaning operations on text.\"\"\"\n",
    "    text = re.sub(r\"[\\'\\(\\)]\", '', text)  # Remove specific characters\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Newlines to space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()  # Multiple spaces to single space\n",
    "\n",
    "def preprocess_query_final(query, max_tokens=10):\n",
    "    \"\"\"Preprocesses a single query by tokenizing, normalizing, removing stop words, and limiting to a maximum number of tokens.\"\"\"\n",
    "    if not query:\n",
    "        raise ValueError(\"Input query must be a non-empty string\")\n",
    "    query = clean_text(query)\n",
    "    tokens = word_tokenize(query)\n",
    "    tokens = [re.sub(r'\\W+', '', token.lower()) for token in tokens if re.sub(r'\\W+', '', token.lower())]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens = tokens[:max_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_rewritten_queries(rewritten_queries):\n",
    "    \"\"\"\n",
    "    Preprocesses the rewritten queries to extract useful information.\n",
    "\n",
    "    :param rewritten_queries: A list of dictionaries with 'generated_text' keys.\n",
    "    :return: A list of cleaned, concise queries.\n",
    "    \"\"\"\n",
    "    cleaned_queries = []\n",
    "\n",
    "    for query in rewritten_queries:\n",
    "        # Extract the generated text\n",
    "        generated_text = query.get('generated_text', '')\n",
    "\n",
    "        # Clean up the generated text by removing unwanted characters\n",
    "        generated_text = re.sub(r'\\n+', ' ', generated_text)  # Replace one or more newlines with a single space\n",
    "        generated_text = re.sub(r'\\s+', ' ', generated_text).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "        # Remove instructional text and formatting tags\n",
    "        useful_text = re.sub(r\"<s> \\[INST\\].*?\\[/INST\\]</s>\", \"\", generated_text, flags=re.DOTALL)\n",
    "\n",
    "        # Split on line breaks or common dividers and select the first non-empty line\n",
    "        potential_queries = re.split(r\"\\n\\nOR\\n\\n|;\", useful_text)\n",
    "        potential_queries = [q.strip() for q in potential_queries if q.strip()]\n",
    "\n",
    "        # Choose the first non-empty, concise piece of text\n",
    "        if potential_queries:\n",
    "            cleaned_queries.append(potential_queries[0])\n",
    "\n",
    "    return cleaned_queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:44:01.581843Z",
     "start_time": "2024-03-30T07:44:01.319864Z"
    }
   },
   "id": "812e513be3f99154",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_id):\n",
    "        self.dataset_id = dataset_id\n",
    "        if not pt.started():\n",
    "            pt.init()\n",
    "        self.dataset = pt.get_dataset(dataset_id)\n",
    "        self.topics = self.dataset.get_topics()\n",
    "        self.qrels = self.dataset.get_qrels()\n",
    "        self.corpus_iter = self.dataset.get_corpus_iter()\n",
    "        self.corpus_iterator = iter(self.corpus_iter)\n",
    "\n",
    "    def get_first_doc(self):\n",
    "        return next(self.corpus_iterator)\n",
    "\n",
    "    # Assuming each topic includes a 'query_id' and 'query' field\n",
    "    def get_original_queries(self):\n",
    "        return [(topic['qid'], topic['query']) for topic_id, topic in\n",
    "                self.topics.iterrows()]\n",
    "\n",
    "\n",
    "class QueryEvaluator:\n",
    "    def __init__(self, tokenizer_model, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate_queries(self, sentences):\n",
    "        features = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(**features).logits\n",
    "        return scores[:, 0]\n",
    "\n",
    "# TODO: We need to adjust this class so that it gives reasonable outputs for the queries\n",
    "class RewriteQueries:\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    def __init__(self, auth_token):\n",
    "        self.headers = {\"Authorization\": f\"Bearer {auth_token}\"}\n",
    "\n",
    "    def query(self, query):\n",
    "        prompt = f\"<s> [INST] Concisely rewrite this into a search engine query: '{query}'. Aim for brevity and clarity without further explanation. [/INST]</s>\"\n",
    "        prompt = {\"inputs\": prompt}\n",
    "        response = requests.post(self.API_URL, headers=self.headers, json=prompt)\n",
    "        return response.json()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:44:01.587155Z",
     "start_time": "2024-03-30T07:44:01.582697Z"
    }
   },
   "id": "38b8c30a7f1a5810",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-passage/trec-dl-2020 documents:   0%|          | 0/8841823 [00:18<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = DatasetLoader('irds:msmarco-passage/trec-dl-2020')\n",
    "original_queries = dataset_loader.get_original_queries()\n",
    "original_queries_df = pd.DataFrame(original_queries, columns=['qid', 'query'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:58:24.052419Z",
     "start_time": "2024-03-30T07:58:24.017284Z"
    }
   },
   "id": "be634c178eb12c70",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Ashishkr/query_wellformedness_score were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 96 queries to rewrite.\n",
      "[(('1037496', 'who is rep scalise'), tensor(0.1841)), (('1051399', 'who sings monk theme song'), tensor(0.3793)), (('1103791', 'definition of endorsing'), tensor(0.2150)), (('1105792', 'define geon'), tensor(0.2947)), (('1105860', 'where can the amazon rainforest is located'), tensor(0.1853)), (('1106979', 'define pareto chart in statistics'), tensor(0.1922)), (('1108450', 'define define gallows'), tensor(0.1133)), (('1108651', 'what the best way to get clothes white'), tensor(0.1986)), (('1108729', 'what temperature and humidity to dry sausage'), tensor(0.1304)), (('1109699', 'what mental illnesses'), tensor(0.0420)), (('1110678', 'what is the un fao'), tensor(0.2620)), (('1113042', 'what is shakespeare s theatre called'), tensor(0.2335)), (('1113256', 'what is reba mcentire s net worth'), tensor(0.3470)), (('1114993', 'conformative definition'), tensor(0.1109)), (('1118370', 'what does provisions mean'), tensor(0.2031)), (('1120588', 'caries detection system'), tensor(0.1249)), (('1121879', 'what are the major political parties in great britain select all that apply'), tensor(0.0669)), (('1122138', 'what are symptoms of kid'), tensor(0.1188)), (('1123657', 'walmart phone number linton in'), tensor(0.0332)), (('1125632', 'routing number for savings bank of maine'), tensor(0.2625)), (('1125755', 'reigning definition'), tensor(0.0872)), (('1126523', 'outmaneuver definition'), tensor(0.1248)), (('1126738', 'number of employees for disa global solutions'), tensor(0.2465)), (('1127004', 'ms symptoms ms'), tensor(0.0930)), (('1127233', 'monk meaning'), tensor(0.0352)), (('1127540', 'meaning of shebang'), tensor(0.2113)), (('1130705', 'how much do a passport'), tensor(0.0205)), (('1130847', 'how many years you have to go through and school is to become a anesthesiologist'), tensor(0.0910)), (('1131069', 'how many sons robert kraft has'), tensor(0.0098)), (('1132044', 'how many bricks per wall'), tensor(0.0892)), (('1132532', 'average annual income data analyst'), tensor(0.0661)), (('1133485', 'how does vitamin c helps'), tensor(0.0552)), (('1134094', 'hotshot members'), tensor(0.0592)), (('1134207', 'holidays definition'), tensor(0.1131)), (('1134431', 'head basketball coach at texas a m'), tensor(0.2657)), (('1134680', 'geneva il median sales price'), tensor(0.0612)), (('1134939', 'flyover definition'), tensor(0.0811)), (('1134988', 'first eagle credit union routing number'), tensor(0.1303)), (('1135268', 'antibiotics for what kind of infection'), tensor(0.1402)), (('1135283', 'embedded payments definition'), tensor(0.1892)), (('1135413', 'dx code for thoracic outlet syndrome'), tensor(0.2999)), (('1136047', 'difference between a company s strategy and business model is'), tensor(0.2516)), (('118440', 'define bmt medical'), tensor(0.2025)), (('119821', 'define curvilinear'), tensor(0.2134)), (('121171', 'define etruscans'), tensor(0.2567)), (('125659', 'define premature babies'), tensor(0.1978)), (('135802', 'definition of laudable'), tensor(0.1498)), (('144862', 'did prohibition increased crime'), tensor(0.1189)), (('174463', 'dog day afternoon meaning'), tensor(0.0529)), (('175920', 'driving distance geneva ny to syracuse'), tensor(0.0982)), (('177604', 'eating foods that are considered warm'), tensor(0.3353)), (('181626', 'estar meaning'), tensor(0.0661)), (('197312', 'group edit policy'), tensor(0.0392)), (('206106', 'hotels in st louis area'), tensor(0.1531)), (('253749', 'how long does it take for discover to raise limit'), tensor(0.3922)), (('26703', 'army dating websites'), tensor(0.0663)), (('302846', 'how much caffeine in twinings green tea'), tensor(0.0303)), (('318362', 'how much does talent directors get paid a year'), tensor(0.1649)), (('330501', 'how much weight on usps letter'), tensor(0.0212)), (('390360', 'ia suffix meaning'), tensor(0.1292)), (('42255', 'average salary for dental hygienist in nebraska'), tensor(0.3963)), (('425632', 'is the a splitboard in skis'), tensor(0.1565)), (('42752', 'average salary in canada 1985'), tensor(0.1749)), (('436707', 'largest known insects'), tensor(0.0700)), (('444389', 'magnesium definition chemistry'), tensor(0.0965)), (('449367', 'meaning of tattoos around eyes'), tensor(0.3072)), (('452915', 'metabolic disease signs and symptoms'), tensor(0.2113)), (('463271', 'neptune s distance from earth'), tensor(0.0643)), (('469589', 'oracle bind variable example'), tensor(0.0839)), (('47210', 'average wedding dress alteration cost'), tensor(0.3499)), (('482726', 'projective definition'), tensor(0.1225)), (('48792', 'barclays fca number'), tensor(0.3108)), (('50122', 'benefit policy in layoff'), tensor(0.0229)), (('514096', 'the after hours clinic'), tensor(0.0942)), (('519025', 'the symptoms of shingles'), tensor(0.2332)), (('53233', 'biggest loser challenge'), tensor(0.0732)), (('537060', 'village of burnham'), tensor(0.3784)), (('537817', 'vitamin e anti scar'), tensor(0.0787)), (('543273', 'weather in antigua november'), tensor(0.1494)), (('545355', 'weather in novi sad'), tensor(0.0520)), (('583468', 'what carvedilol used for'), tensor(0.0261)), (('610265', 'what county is new york new york in'), tensor(0.1448)), (('673670', 'what is a alm'), tensor(0.1756)), (('708979', 'what is aids and hiv'), tensor(0.1456)), (('730539', 'what is chronometer who invented it'), tensor(0.1114)), (('735482', 'what is cow chip t'), tensor(0.3724)), (('75198', 'can uti cause stroke'), tensor(0.3725)), (('768208', 'what is mamey'), tensor(0.3226)), (('849550', 'what is the symptoms of croup'), tensor(0.1927)), (('85020', 'causes for shortness of breath through exertion'), tensor(0.3119)), (('86606', 'causes of gas in large intestine'), tensor(0.2439)), (('88495', 'causes of stroke'), tensor(0.2197)), (('911232', 'what type of conflict does della face in o henry the gift of the magi'), tensor(0.1483)), (('91576', 'chicken as food wikipedia'), tensor(0.0428)), (('99005', 'convert sq meter to sq inch'), tensor(0.1898)), (('132622', 'definition of attempted arson'), tensor(0.3279))]\n"
     ]
    }
   ],
   "source": [
    "query_evaluator = QueryEvaluator(\"Ashishkr/query_wellformedness_score\", \"Ashishkr/query_wellformedness_score\")\n",
    "\n",
    "selected_threshold = 0.4\n",
    "\n",
    "query_texts = [query_text for qid, query_text in original_queries]\n",
    "\n",
    "# Evaluate the well-formedness scores of the extracted query texts\n",
    "well_formed_scores = query_evaluator.evaluate_queries(query_texts)\n",
    "\n",
    "queries_to_rewrite = [(original_query, score) for original_query, score in zip(original_queries, well_formed_scores) if score < selected_threshold]\n",
    "\n",
    "print(f\"We have {len(queries_to_rewrite)} queries to rewrite.\")\n",
    "print(queries_to_rewrite[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:44:03.617962Z",
     "start_time": "2024-03-30T07:44:02.252141Z"
    }
   },
   "id": "ecf4af08256ab439",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Modified section for handling rewritten queries to maintain original query association\n",
    "rewritten_queries = load_data_from_file(rewritten_queries_path)\n",
    "if rewritten_queries is None:\n",
    "    rewritten_queries = []\n",
    "    query_rewriter = RewriteQueries(\"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\")\n",
    "    for original_query, _ in queries_to_rewrite:  # original_query is a tuple (qid, query_text)\n",
    "        qid, query_text = original_query  # Unpack the original_query tuple\n",
    "        response = query_rewriter.query(query_text)  # Pass only query_text for rewriting\n",
    "        if response and isinstance(response, list) and 'generated_text' in response[0]:\n",
    "            generated_text = response[0]['generated_text']\n",
    "            rewritten_queries.append({'original_query': original_query, 'generated_text': generated_text})\n",
    "    save_data_to_file(rewritten_queries, rewritten_queries_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:45:38.109459Z",
     "start_time": "2024-03-30T07:44:03.618951Z"
    }
   },
   "id": "f10586cd42f39f90",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Process the rewritten queries with the original query included\n",
    "cleaned_queries = [(query['original_query'], preprocess_rewritten_queries([query])[0]) for query in rewritten_queries]\n",
    "\n",
    "with open('cleaned_queries_with_original.json', 'w') as f:\n",
    "    json.dump(cleaned_queries, f)\n",
    "\n",
    "for original_query, cleaned_query in cleaned_queries:\n",
    "    preprocessed_query = preprocess_query(cleaned_query)\n",
    "    final_query = remove_redundancy(preprocessed_query)\n",
    "\n",
    "\n",
    "# output the final queries to a JSON file\n",
    "with open('final_queries.json', 'w') as f:\n",
    "    json.dump(cleaned_queries, f)\n",
    "\n",
    "\n",
    "# Load the final queries with original and rewritten versions\n",
    "with open('final_queries.json', 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "# Load the cleaned and potentially rewritten queries\n",
    "with open('cleaned_queries_with_original.json', 'r') as f:\n",
    "    cleaned_queries = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:45:38.373712Z",
     "start_time": "2024-03-30T07:45:38.111568Z"
    }
   },
   "id": "7caaba0d6cc5eef0",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       qid                                    original  \\\n0  1037496                          who is rep scalise   \n1  1051399                   who sings monk theme song   \n2  1103791                     definition of endorsing   \n3  1105792                                 define geon   \n4  1105860  where can the amazon rainforest is located   \n\n                                           rewritten  \n0  Representative Steve Scalise biography or back...  \n1  ```sql Monk theme song singer ``` or ``` Monk ...  \n2       \"Endorsing definition\" or \"define endorsing\"  \n3             \"Geon definition\" or \"What is a geon?\"  \n4  \"Amazon rainforest location\" This query is bri...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>original</th>\n      <th>rewritten</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1037496</td>\n      <td>who is rep scalise</td>\n      <td>Representative Steve Scalise biography or back...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1051399</td>\n      <td>who sings monk theme song</td>\n      <td>```sql Monk theme song singer ``` or ``` Monk ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1103791</td>\n      <td>definition of endorsing</td>\n      <td>\"Endorsing definition\" or \"define endorsing\"</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1105792</td>\n      <td>define geon</td>\n      <td>\"Geon definition\" or \"What is a geon?\"</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1105860</td>\n      <td>where can the amazon rainforest is located</td>\n      <td>\"Amazon rainforest location\" This query is bri...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract q_id and original query into separate lists\n",
    "q_ids, original_queries = zip(*[ (q_id, query) for (q_id, query), _ in cleaned_queries ])\n",
    "rewritten_queries = [rewritten for _, rewritten in cleaned_queries]\n",
    "\n",
    "# Create a DataFrame with separate columns for q_id, original, and rewritten\n",
    "df_queries = pd.DataFrame({\n",
    "    'qid': q_ids,\n",
    "    'original': original_queries,\n",
    "    'rewritten': rewritten_queries\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_queries.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:58:51.264431Z",
     "start_time": "2024-03-30T07:58:51.256629Z"
    }
   },
   "id": "4c2be1b3c5ef3f5f",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       qid                                       query\n0  1030303                          who is aziz hashim\n1  1037496                          who is rep scalise\n2  1043135            who killed nicholas ii of russia\n3  1045109                     who owns barnhart crane\n4  1049519  who said no one can make you feel inferior",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1030303</td>\n      <td>who is aziz hashim</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1037496</td>\n      <td>who is rep scalise</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1043135</td>\n      <td>who killed nicholas ii of russia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1045109</td>\n      <td>who owns barnhart crane</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1049519</td>\n      <td>who said no one can make you feel inferior</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_queries_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:58:30.122068Z",
     "start_time": "2024-03-30T07:58:30.115238Z"
    }
   },
   "id": "81f74c68a80b18a1",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       qid                                              query\n0  1030303                                 who is aziz hashim\n1  1037496  Representative Steve Scalise biography or back...\n2  1043135                   who killed nicholas ii of russia\n3  1045109                            who owns barnhart crane\n4  1049519         who said no one can make you feel inferior",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>query</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1030303</td>\n      <td>who is aziz hashim</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1037496</td>\n      <td>Representative Steve Scalise biography or back...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1043135</td>\n      <td>who killed nicholas ii of russia</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1045109</td>\n      <td>who owns barnhart crane</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1049519</td>\n      <td>who said no one can make you feel inferior</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming df_queries and original_queries_df are already defined DataFrames as shown in the images provided\n",
    "# Merge the two DataFrames on 'q_id'\n",
    "merged_df = pd.merge(original_queries_df, df_queries, on='qid', how='left')\n",
    "\n",
    "# Replace NaNs in the 'rewritten' column with the 'original' query from the original_queries_df\n",
    "merged_df['rewritten'] = merged_df['rewritten'].fillna(merged_df['query'])\n",
    "\n",
    "# If you want to rename the 'query' column to 'original' for consistency\n",
    "merged_df = merged_df.rename(columns={'query': 'original_y'})\n",
    "\n",
    "# Now you can drop any redundant columns if they exist (assuming 'original_y' is redundant)\n",
    "merged_df = merged_df.drop(columns=['original', 'original_y'], errors='ignore')\n",
    "\n",
    "# rename the rewritten column to query\n",
    "rewritten_queries_df = merged_df.rename(columns={'rewritten': 'query'})\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "rewritten_queries_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:03:33.771188Z",
     "start_time": "2024-03-30T08:03:33.761326Z"
    }
   },
   "id": "474214f826c0bede",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists, loading from disk.\n"
     ]
    }
   ],
   "source": [
    "index_location = str(Path(\"index\").absolute())\n",
    "index_exists = os.path.isfile(\n",
    "    os.path.join(index_location, \"data.properties\"))\n",
    "\n",
    "# Fetch corpus iterator just before indexing\n",
    "if not index_exists:\n",
    "    corpus_iter = dataset_loader.corpus_iter \n",
    "    indexer = pt.IterDictIndexer(index_location)\n",
    "    index_ref = indexer.index(corpus_iter)\n",
    "    print(\"Indexing completed.\")\n",
    "else:\n",
    "    print(\"Index already exists, loading from disk.\")\n",
    "    index_ref = index_location\n",
    "\n",
    "# Assuming qrels are loaded correctly\n",
    "qrels = dataset_loader.qrels\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "eval_metrics = [\n",
    "    pt.measures.RR(rel=1),\n",
    "    pt.measures.nDCG @ 10,\n",
    "    pt.measures.MAP(rel=1),\n",
    "    pt.measures.Precision @ 5,  # Precision at rank 5\n",
    "    pt.measures.Recall @ 100,   # Recall at rank 100\n",
    "    pt.measures.MRR             # Mean Reciprocal Rank\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T08:04:02.338548Z",
     "start_time": "2024-03-30T08:04:01.653124Z"
    }
   },
   "id": "d6ccbebb0bc5addd",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Original Queries with BM25 and TF-IDF:\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Original Queries\n",
    "print(\"Evaluating Original Queries with BM25 and TF-IDF:\")\n",
    "results_original = pt.Experiment(\n",
    "    [bm25, tf_idf],  # List of retrieval systems to evaluate\n",
    "    original_queries_df[['qid', 'query']],  # DataFrame with queries\n",
    "    qrels,  # Qrels for relevance judgments\n",
    "    eval_metrics, \n",
    "    names=[\"BM25 Original\", \"TF-IDF Original\"]  # Names for the systems\n",
    ")\n",
    "\n",
    "print(f\"Results for Original Queries:\\n{results_original}\")\n",
    "results_original.to_csv('results_original.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-30T08:04:07.236307Z"
    }
   },
   "id": "42b33d2795bd1a9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(\"Evaluating Rewritten Queries with BM25 and TF-IDF:\")\n",
    "simple_results = pt.Experiment(\n",
    "    [bm25, tf_idf],\n",
    "    rewritten_queries_df[['qid', 'query']],\n",
    "    qrels,\n",
    "    eval_metrics,\n",
    "    names=[\"BM25 Rewritten\", \"TF-IDF Rewritten\"]\n",
    ")\n",
    "\n",
    "print(f\"Results for Rewritten Queries:\\n{simple_results}\")\n",
    "simple_results.to_csv('simple_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e3f93451f1692c1a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6542c47fef9ffbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
