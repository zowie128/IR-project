{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:23:16.793371Z",
     "start_time": "2024-03-30T07:23:12.836079Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyterrier as pt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Token\n",
    "\n",
    "\n",
    "# Ensure NLTK data is downloaded only once\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Define file paths\n",
    "original_queries_path = 'original_queries.json'\n",
    "rewritten_queries_path = 'rewritten_queries.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Functions to save and load data\n",
    "def save_data_to_file(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def load_data_from_file(file_path):\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    return None\n",
    "\n",
    "def remove_redundancy(preprocessed_query):\n",
    "    tokens = preprocessed_query.split()\n",
    "    seen = set()\n",
    "    unique_tokens = [t for t in tokens if not (t in seen or seen.add(t))]\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "# Load spaCy English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extend stop words list with common but low-information words\n",
    "extended_stop_words = {\"define\", \"meaning\", \"example\", \"describe\", \"use\", \"refer\", \"relate\", \"involve\", \"include\", \"give\", \"take\", \"make\", \"see\", \"want\", \"get\", \"say\", \"ask\", \"tell\", \"be\", \"know\", \"do\", \"have\", \"would\", \"should\", \"could\", \"about\"}\n",
    "for word in extended_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "# Customize token extension to flag important tokens to keep\n",
    "Token.set_extension(\"is_important\", default=False, force=True)\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Preprocess a single query using spaCy for tokenization, lemmatization, and stop word removal,\n",
    "    aiming for greater conciseness.\n",
    "    \"\"\"\n",
    "    # Clean up the query by removing unwanted characters\n",
    "    query = re.sub(r'\\n+', ' ', query)  # Replace one or more newlines with a single space\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(query)\n",
    "\n",
    "    # Identify important tokens to preserve\n",
    "    for ent in doc.ents:\n",
    "        for token in ent:\n",
    "            token._.is_important = True\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"PROPN\", \"NOUN\", \"VERB\"}:\n",
    "            token._.is_important = True\n",
    "\n",
    "    # Condense the query by keeping important tokens and removing less important ones\n",
    "    tokens = [token.lemma_.lower() for token in doc if (token._.is_important or token.text.lower() in extended_stop_words) and not token.is_stop and token.pos_ != \"PUNCT\"]\n",
    "\n",
    "    # Reconstruct the query\n",
    "    preprocessed_query = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_query\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Performs common cleaning operations on text.\"\"\"\n",
    "    text = re.sub(r\"[\\'\\(\\)]\", '', text)  # Remove specific characters\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Newlines to space\n",
    "    return re.sub(r'\\s+', ' ', text).strip()  # Multiple spaces to single space\n",
    "\n",
    "def preprocess_query_final(query, max_tokens=10):\n",
    "    \"\"\"Preprocesses a single query by tokenizing, normalizing, removing stop words, and limiting to a maximum number of tokens.\"\"\"\n",
    "    if not query:\n",
    "        raise ValueError(\"Input query must be a non-empty string\")\n",
    "    query = clean_text(query)\n",
    "    tokens = word_tokenize(query)\n",
    "    tokens = [re.sub(r'\\W+', '', token.lower()) for token in tokens if re.sub(r'\\W+', '', token.lower())]\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens = tokens[:max_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_rewritten_queries(rewritten_queries):\n",
    "    \"\"\"\n",
    "    Preprocesses the rewritten queries to extract useful information.\n",
    "\n",
    "    :param rewritten_queries: A list of dictionaries with 'generated_text' keys.\n",
    "    :return: A list of cleaned, concise queries.\n",
    "    \"\"\"\n",
    "    cleaned_queries = []\n",
    "\n",
    "    for query in rewritten_queries:\n",
    "        # Extract the generated text\n",
    "        generated_text = query.get('generated_text', '')\n",
    "\n",
    "        # Clean up the generated text by removing unwanted characters\n",
    "        generated_text = re.sub(r'\\n+', ' ', generated_text)  # Replace one or more newlines with a single space\n",
    "        generated_text = re.sub(r'\\s+', ' ', generated_text).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "        # Remove instructional text and formatting tags\n",
    "        useful_text = re.sub(r\"<s> \\[INST\\].*?\\[/INST\\]</s>\", \"\", generated_text, flags=re.DOTALL)\n",
    "\n",
    "        # Split on line breaks or common dividers and select the first non-empty line\n",
    "        potential_queries = re.split(r\"\\n\\nOR\\n\\n|;\", useful_text)\n",
    "        potential_queries = [q.strip() for q in potential_queries if q.strip()]\n",
    "\n",
    "        # Choose the first non-empty, concise piece of text\n",
    "        if potential_queries:\n",
    "            cleaned_queries.append(potential_queries[0])\n",
    "\n",
    "    return cleaned_queries"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:23:17.075401Z",
     "start_time": "2024-03-30T07:23:16.794704Z"
    }
   },
   "id": "812e513be3f99154",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_id):\n",
    "        self.dataset_id = dataset_id\n",
    "        if not pt.started():\n",
    "            pt.init()\n",
    "        self.dataset = pt.get_dataset(dataset_id)\n",
    "        self.topics = self.dataset.get_topics()\n",
    "        self.qrels = self.dataset.get_qrels()\n",
    "        self.corpus_iter = self.dataset.get_corpus_iter()\n",
    "        self.corpus_iterator = iter(self.corpus_iter)\n",
    "\n",
    "    def get_first_doc(self):\n",
    "        return next(self.corpus_iterator)\n",
    "\n",
    "    # Assuming each topic includes a 'query_id' and 'query' field\n",
    "    def get_original_queries(self):\n",
    "        return [(topic['qid'], topic['query']) for topic_id, topic in\n",
    "                self.topics.iterrows()]\n",
    "\n",
    "\n",
    "class QueryEvaluator:\n",
    "    def __init__(self, tokenizer_model, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate_queries(self, sentences):\n",
    "        features = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(**features).logits\n",
    "        return scores[:, 0]\n",
    "\n",
    "# TODO: We need to adjust this class so that it gives reasonable outputs for the queries\n",
    "class RewriteQueries:\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    def __init__(self, auth_token):\n",
    "        self.headers = {\"Authorization\": f\"Bearer {auth_token}\"}\n",
    "\n",
    "    def query(self, query):\n",
    "        prompt = f\"<s> [INST] Concisely rewrite this into a search engine query: '{query}'. Aim for brevity and clarity without further explanation. [/INST]</s>\"\n",
    "        prompt = {\"inputs\": prompt}\n",
    "        response = requests.post(self.API_URL, headers=self.headers, json=prompt)\n",
    "        return response.json()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:23:17.080486Z",
     "start_time": "2024-03-30T07:23:17.076310Z"
    }
   },
   "id": "38b8c30a7f1a5810",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n",
      "msmarco-passage/trec-dl-2020 documents:   0%|          | 0/8841823 [00:00<?, ?it/s]Some weights of the model checkpoint at Ashishkr/query_wellformedness_score were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 122 queries to rewrite.\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = DatasetLoader('irds:msmarco-passage/trec-dl-2020')\n",
    "original_queries = dataset_loader.get_original_queries()\n",
    "original_queries_df = pd.DataFrame(original_queries, columns=['qid', 'query'])\n",
    "\n",
    "# Evaluate queries to determine which need rewriting\n",
    "query_evaluator = QueryEvaluator(\"Ashishkr/query_wellformedness_score\", \"Ashishkr/query_wellformedness_score\")\n",
    "well_formed_scores = query_evaluator.evaluate_queries(original_queries)\n",
    "selected_threshold = 0.4\n",
    "queries_to_rewrite = [(query, score) for query, score in zip(original_queries, well_formed_scores) if score < selected_threshold]\n",
    "\n",
    "print(f\"We have {len(queries_to_rewrite)} queries to rewrite.\")\n",
    "\n",
    "# Modified section for handling rewritten queries to maintain original query association\n",
    "rewritten_queries = load_data_from_file(rewritten_queries_path)\n",
    "if rewritten_queries is None:\n",
    "    rewritten_queries = []\n",
    "    query_rewriter = RewriteQueries(\"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\")\n",
    "    for original_query, _ in queries_to_rewrite:\n",
    "        response = query_rewriter.query(original_query)\n",
    "        if response and isinstance(response, list) and 'generated_text' in response[0]:\n",
    "            generated_text = response[0]['generated_text']\n",
    "            rewritten_queries.append({'original_query': original_query, 'generated_text': generated_text})\n",
    "    save_data_to_file(rewritten_queries, rewritten_queries_path)\n",
    "\n",
    "# Process the rewritten queries with the original query included\n",
    "cleaned_queries = [(query['original_query'], preprocess_rewritten_queries([query])[0]) for query in rewritten_queries]\n",
    "\n",
    "with open('cleaned_queries_with_original.json', 'w') as f:\n",
    "    json.dump(cleaned_queries, f)\n",
    "\n",
    "for original_query, cleaned_query in cleaned_queries:\n",
    "    preprocessed_query = preprocess_query(cleaned_query)\n",
    "    final_query = remove_redundancy(preprocessed_query)\n",
    "    # print(f\"Original Query: {original_query} | Final Query: {final_query}\")\n",
    "\n",
    "\n",
    "# output the final queries to a JSON file\n",
    "with open('final_queries.json', 'w') as f:\n",
    "    json.dump(cleaned_queries, f)\n",
    "\n",
    "\n",
    "# Load the final queries with original and rewritten versions\n",
    "with open('final_queries.json', 'r') as f:\n",
    "    queries_data = json.load(f)\n",
    "\n",
    "# Load the cleaned and potentially rewritten queries\n",
    "with open('cleaned_queries_with_original.json', 'r') as f:\n",
    "    cleaned_queries = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:23:20.961390Z",
     "start_time": "2024-03-30T07:23:17.081576Z"
    }
   },
   "id": "be634c178eb12c70",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       qid                                    original  \\\n",
      "0  1030303                          who is aziz hashim   \n",
      "1  1037496                          who is rep scalise   \n",
      "2  1043135            who killed nicholas ii of russia   \n",
      "3  1045109                     who owns barnhart crane   \n",
      "4  1049519  who said no one can make you feel inferior   \n",
      "\n",
      "                                               query  \n",
      "0                                        aziz hashim  \n",
      "1  c rep scalise background information steve sca...  \n",
      "2                          killed nicholas ii russia  \n",
      "3                                owns barnhart crane  \n",
      "4                        said one make feel inferior  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_3/jggbxqpj77x_m8nr3ll39p5r0000gn/T/ipykernel_26836/2011718922.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_queries_aligned['rewritten'].fillna(df_queries_aligned['query'],\n"
     ]
    }
   ],
   "source": [
    "# Convert the loaded data into a DataFrame\n",
    "# This assumes cleaned_queries now includes q_id in each tuple\n",
    "df_queries = pd.DataFrame(cleaned_queries,\n",
    "                          columns=['original', 'rewritten'])\n",
    "\n",
    "# Perform a left merge to include all original queries\n",
    "df_queries_aligned = pd.merge(original_queries_df, df_queries,\n",
    "                              left_on='query', right_on='original',\n",
    "                              how='left')\n",
    "\n",
    "# Fill NaN values in 'rewritten' with the 'query' values\n",
    "df_queries_aligned['rewritten'].fillna(df_queries_aligned['query'],\n",
    "                                       inplace=True)\n",
    "df_queries_aligned.drop('original', axis=1, inplace=True)\n",
    "df_queries_aligned.rename(columns={'query': 'original'}, inplace=True)\n",
    "df_queries_aligned.rename(columns={'rewritten': 'query'}, inplace=True)\n",
    "# Rename 'q_id' column to 'qid' to align with PyTerrier's expectations\n",
    "df_queries_aligned.rename(columns={'q_id': 'qid'}, inplace=True)\n",
    "\n",
    "# run preprocess final queries on df_queries_aligned: query column\n",
    "df_queries_aligned['query'] = df_queries_aligned['query'].apply(preprocess_query_final)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:23:21.120932Z",
     "start_time": "2024-03-30T07:23:20.963047Z"
    }
   },
   "id": "4c2be1b3c5ef3f5f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index already exists, loading from disk.\n"
     ]
    }
   ],
   "source": [
    "index_location = str(Path(\"index\").absolute())\n",
    "index_exists = os.path.isfile(\n",
    "    os.path.join(index_location, \"data.properties\"))\n",
    "\n",
    "# Fetch corpus iterator just before indexing\n",
    "if not index_exists:\n",
    "    corpus_iter = dataset_loader.corpus_iter  # Adjusted from get_corpus_iter() to corpus_iter\n",
    "    indexer = pt.IterDictIndexer(index_location)\n",
    "    index_ref = indexer.index(corpus_iter)\n",
    "    print(\"Indexing completed.\")\n",
    "else:\n",
    "    print(\"Index already exists, loading from disk.\")\n",
    "    index_ref = index_location\n",
    "\n",
    "# Assuming qrels are loaded correctly\n",
    "qrels = dataset_loader.qrels\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "eval_metrics = [\n",
    "    pt.measures.RR(rel=1),\n",
    "    pt.measures.nDCG @ 10,\n",
    "    pt.measures.MAP(rel=1),\n",
    "    pt.measures.Precision @ 5,  # Precision at rank 5\n",
    "    pt.measures.Recall @ 100,   # Recall at rank 100\n",
    "    pt.measures.MRR             # Mean Reciprocal Rank\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:28:24.036145Z",
     "start_time": "2024-03-30T07:28:23.189644Z"
    }
   },
   "id": "d6ccbebb0bc5addd",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Original Queries with BM25 and TF-IDF:\n",
      "Results for Original Queries:\n",
      "              name        RR   nDCG@10        AP       P@5     R@100\n",
      "0    BM25 Original  0.802359  0.493627  0.358724  0.625926  0.504658\n",
      "1  TF-IDF Original  0.802359  0.492575  0.358072  0.625926  0.503206\n"
     ]
    }
   ],
   "source": [
    "# Evaluating Original Queries\n",
    "print(\"Evaluating Original Queries with BM25 and TF-IDF:\")\n",
    "results_original = pt.Experiment(\n",
    "    [bm25, tf_idf],  # List of retrieval systems to evaluate\n",
    "    original_queries_df[['qid', 'query']],  # DataFrame with queries\n",
    "    qrels,  # Qrels for relevance judgments\n",
    "    eval_metrics, \n",
    "    names=[\"BM25 Original\", \"TF-IDF Original\"]  # Names for the systems\n",
    ")\n",
    "\n",
    "print(f\"Results for Original Queries:\\n{results_original}\")\n",
    "results_original.to_csv('results_original.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:28:50.368057Z",
     "start_time": "2024-03-30T07:28:24.037244Z"
    }
   },
   "id": "42b33d2795bd1a9",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Rewritten Queries with BM25 and TF-IDF:\n",
      "Results for Rewritten Queries:\n",
      "               name        RR   nDCG@10        AP       P@5     R@100\n",
      "0    BM25 Rewritten  0.768337  0.470668  0.333378  0.592593  0.477283\n",
      "1  TF-IDF Rewritten  0.777925  0.470496  0.333009  0.592593  0.476240\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating Rewritten Queries with BM25 and TF-IDF:\")\n",
    "simple_results = pt.Experiment(\n",
    "    [bm25, tf_idf],\n",
    "    df_queries_aligned[['qid', 'query']],\n",
    "    qrels,\n",
    "    eval_metrics,\n",
    "    names=[\"BM25 Rewritten\", \"TF-IDF Rewritten\"]\n",
    ")\n",
    "\n",
    "print(f\"Results for Rewritten Queries:\\n{simple_results}\")\n",
    "simple_results.to_csv('simple_results.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-30T07:29:17.665441Z",
     "start_time": "2024-03-30T07:28:50.369277Z"
    }
   },
   "id": "e3f93451f1692c1a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6542c47fef9ffbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
