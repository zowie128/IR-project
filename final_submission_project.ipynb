{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:51:57.186616Z",
     "start_time": "2024-03-30T08:51:53.762489Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyterrier as pt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.tokens import Token\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import BERTScorer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure NLTK data is downloaded only once\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "# Load stop words once and pass them to the preprocess function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define file path\n",
    "rewritten_queries_path = 'rewritten_queries.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "812e513be3f99154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:36:52.948761Z",
     "start_time": "2024-03-30T08:36:52.668374Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to save and load data\n",
    "def save_data_to_file(data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "def load_data_from_file(file_path):\n",
    "    if Path(file_path).exists():\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    return None\n",
    "\n",
    "def remove_redundancy(preprocessed_query):\n",
    "    tokens = preprocessed_query.split()\n",
    "    seen = set()\n",
    "    unique_tokens = [t for t in tokens if not (t in seen or seen.add(t))]\n",
    "    return ' '.join(unique_tokens)\n",
    "\n",
    "\n",
    "# Load spaCy English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Extend stop words list with common but low-information words\n",
    "extended_stop_words = {\"define\", \"meaning\", \"example\", \"describe\", \"use\", \"refer\", \"relate\", \"involve\", \"include\", \"give\", \"take\", \"make\", \"see\", \"want\", \"get\", \"say\", \"ask\", \"tell\", \"be\", \"know\", \"do\", \"have\", \"would\", \"should\", \"could\", \"about\"}\n",
    "for word in extended_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "\n",
    "# Customize token extension to flag important tokens to keep\n",
    "Token.set_extension(\"is_important\", default=False, force=True)\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Preprocess a single query using spaCy for tokenization, lemmatization, and stop word removal,\n",
    "    aiming for greater conciseness.\n",
    "    \"\"\"\n",
    "    # Clean up the query by removing unwanted characters\n",
    "    query = re.sub(r'\\n+', ' ', query)  # Replace one or more newlines with a single space\n",
    "    query = re.sub(r'\\s+', ' ', query).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "    # Process the text\n",
    "    doc = nlp(query)\n",
    "\n",
    "    # Identify important tokens to preserve\n",
    "    for ent in doc.ents:\n",
    "        for token in ent:\n",
    "            token._.is_important = True\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"PROPN\", \"NOUN\", \"VERB\"}:\n",
    "            token._.is_important = True\n",
    "\n",
    "    # Condense the query by keeping important tokens and removing less important ones\n",
    "    tokens = [token.lemma_.lower() for token in doc if (token._.is_important or token.text.lower() in extended_stop_words) and not token.is_stop and token.pos_ != \"PUNCT\"]\n",
    "\n",
    "    # Reconstruct the query\n",
    "    preprocessed_query = \" \".join(tokens)\n",
    "\n",
    "    return preprocessed_query\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Performs common cleaning operations on text, including lowering case.\"\"\"\n",
    "    text = re.sub(r\"[\\'\\(\\)]\", '', text)  # Remove specific characters\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Newlines to space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Multiple spaces to single space\n",
    "    return text.lower()  # Convert text to lowercase for uniformity\n",
    "\n",
    "\n",
    "def preprocess_query_final(query, stop_words, max_tokens=10):\n",
    "    \"\"\"Preprocesses a single query by tokenizing, normalizing, removing stop words, and limiting to a maximum number of tokens.\"\"\"\n",
    "    if not query:\n",
    "        raise ValueError(\"Input query must be a non-empty string\")\n",
    "    query = clean_text(query)  # Clean and lower case\n",
    "    tokens = word_tokenize(query)\n",
    "    tokens = [re.sub(r'\\W+', '', token) for token in tokens if re.sub(r'\\W+', '', token)]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = tokens[:max_tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def preprocess_rewritten_query(rewritten_query):\n",
    "    # Extract the generated text\n",
    "    generated_text = rewritten_query.get('generated_text', '')\n",
    "    \n",
    "    # Clean up the generated text by removing unwanted characters\n",
    "    generated_text = re.sub(r'\\n+', ' ', generated_text)  # Replace one or more newlines with a single space\n",
    "    generated_text = re.sub(r'\\s+', ' ', generated_text).strip()  # Replace multiple spaces with a single space and trim\n",
    "    \n",
    "    # Remove instructional text and formatting tags\n",
    "    useful_text = re.sub(r\"<s> \\[INST\\].*?\\[/INST\\]</s>\", \"\", generated_text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove leading and trailing single quotes\n",
    "    useful_text = re.sub(r'(^\\s*\\'|\\'$)', '', useful_text)\n",
    "    \n",
    "    # Split on line breaks or common dividers and select the first non-empty line\n",
    "    potential_queries = re.split(r\"\\n\\nOR\\n\\n|;\", useful_text)\n",
    "    potential_queries = [q.strip() for q in potential_queries if q.strip()]\n",
    "    \n",
    "    # Choose the first non-empty, concise piece of text\n",
    "    if potential_queries:\n",
    "        return potential_queries[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "def preprocess_rewritten_queries(rewritten_queries):\n",
    "    \"\"\"\n",
    "    Preprocesses the rewritten queries to extract useful information.\n",
    "\n",
    "    :param rewritten_queries: A list of dictionaries with 'generated_text' keys.\n",
    "    :return: A list of cleaned, concise queries.\n",
    "    \"\"\"\n",
    "    cleaned_queries = []\n",
    "\n",
    "    for query in rewritten_queries:\n",
    "        # Extract the generated text\n",
    "        generated_text = query.get('generated_text', '')\n",
    "\n",
    "        # Clean up the generated text by removing unwanted characters\n",
    "        generated_text = re.sub(r'\\n+', ' ', generated_text)  # Replace one or more newlines with a single space\n",
    "        generated_text = re.sub(r'\\s+', ' ', generated_text).strip()  # Replace multiple spaces with a single space and trim\n",
    "\n",
    "        # Remove instructional text and formatting tags\n",
    "        useful_text = re.sub(r\"<s> \\[INST\\].*?\\[/INST\\]</s>\", \"\", generated_text, flags=re.DOTALL)\n",
    "\n",
    "        # Split on line breaks or common dividers and select the first non-empty line\n",
    "        potential_queries = re.split(r\"\\n\\nOR\\n\\n|;\", useful_text)\n",
    "        potential_queries = [q.strip() for q in potential_queries if q.strip()]\n",
    "\n",
    "        # Choose the first non-empty, concise piece of text\n",
    "        if potential_queries:\n",
    "            cleaned_queries.append(potential_queries[0])\n",
    "\n",
    "    return cleaned_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38b8c30a7f1a5810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:36:52.953485Z",
     "start_time": "2024-03-30T08:36:52.949407Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_id):\n",
    "        self.dataset_id = dataset_id\n",
    "        if not pt.started():\n",
    "            pt.init()\n",
    "        self.dataset = pt.get_dataset(dataset_id)\n",
    "        self.topics = self.dataset.get_topics()\n",
    "        self.qrels = self.dataset.get_qrels()\n",
    "        self.corpus_iter = self.dataset.get_corpus_iter()\n",
    "        self.corpus_iterator = iter(self.corpus_iter)\n",
    "\n",
    "    def get_first_doc(self):\n",
    "        return next(self.corpus_iterator)\n",
    "\n",
    "    # Assuming each topic includes a 'query_id' and 'query' field\n",
    "    def get_original_queries(self):\n",
    "        return [(topic['qid'], topic['query']) for topic_id, topic in\n",
    "                self.topics.iterrows()]\n",
    "\n",
    "\n",
    "class QueryEvaluator:\n",
    "    def __init__(self, tokenizer_model, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "\n",
    "    def evaluate_queries(self, sentences):\n",
    "        features = self.tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(**features).logits\n",
    "        return scores[:, 0]\n",
    "\n",
    "# TODO: We need to adjust this class so that it gives reasonable outputs for the queries\n",
    "class RewriteQueries:\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "    def __init__(self, auth_token):\n",
    "        self.headers = {\"Authorization\": f\"Bearer {auth_token}\"}\n",
    "\n",
    "    def query(self, query):\n",
    "        prompt = f\"<s> [INST] Concisely rewrite this into a search engine query: {query}. Aim for brevity and clarity without further explanation. [/INST]</s>\"\n",
    "        response = requests.post(self.API_URL, headers=self.headers, json={\"inputs\": prompt})\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be634c178eb12c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:36:53.640128Z",
     "start_time": "2024-03-30T08:36:52.954481Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_loader = DatasetLoader('irds:msmarco-passage/trec-dl-2020')\n",
    "original_queries = dataset_loader.get_original_queries()\n",
    "original_queries_df = pd.DataFrame(original_queries, columns=['qid', 'query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4af08256ab439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:36:55.143107Z",
     "start_time": "2024-03-30T08:36:53.642207Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_evaluator = QueryEvaluator(\"Ashishkr/query_wellformedness_score\", \"Ashishkr/query_wellformedness_score\")\n",
    "\n",
    "selected_threshold = 0.4\n",
    "\n",
    "query_texts = [query_text for _, query_text in original_queries]\n",
    "\n",
    "# Evaluate the well-formedness scores of the extracted query texts\n",
    "well_formed_scores = query_evaluator.evaluate_queries(query_texts)\n",
    "\n",
    "queries_to_rewrite = [(original_query, score) for original_query, score in zip(original_queries, well_formed_scores) if score < selected_threshold]\n",
    "\n",
    "print(f\"Number of queries to rewrite: {len(queries_to_rewrite)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10586cd42f39f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:37:29.316245Z",
     "start_time": "2024-03-30T08:36:55.144085Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Modified section for handling rewritten queries to maintain original query association\n",
    "rewritten_queries = load_data_from_file(rewritten_queries_path)\n",
    "\n",
    "if rewritten_queries is None:\n",
    "    rewritten_queries = []\n",
    "    query_rewriter = RewriteQueries(\"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\")\n",
    "    # query_rewriter = RewriteQueries(\"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\")\n",
    "    t = tqdm(queries_to_rewrite, total=len(queries_to_rewrite), desc=\"Rewriting queries\") # tqdm is used to display a progress bar\n",
    "    for i, (original_query, _) in enumerate(t):  # original_query is a tuple (qid, query_text)\n",
    "        qid, query_text = original_query  # Unpack the original_query tuple\n",
    "        response = query_rewriter.query(query_text)  # Pass only query_text for rewriting\n",
    "        if response and isinstance(response, list) and 'generated_text' in response[0]:\n",
    "            generated_text = response[0]['generated_text']\n",
    "            rewritten_queries.append({'original_query': original_query, 'generated_text': generated_text})\n",
    "        elif response and 'error' in response:\n",
    "            raise ValueError(f\"Error occurred while rewriting query {qid}: {response['error']}\")\n",
    "    \n",
    "    save_data_to_file(rewritten_queries, rewritten_queries_path)\n",
    "\n",
    "# Process the rewritten queries with the original query included\n",
    "cleaned_queries = [(query['original_query'], preprocess_rewritten_query(query)) for query in rewritten_queries]\n",
    "save_data_to_file(cleaned_queries, 'cleaned_queries.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the rewritten queries based on rouge-L and BERT-score\n",
    "rs = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "bs = BERTScorer(model_type='bert-base-uncased')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def rouge_l_score(candidate, reference):\n",
    "    rouge_score = rs.score(reference, candidate)\n",
    "    rouge_p, rouge_r, rouge_f1 = rouge_score['rougeL'].precision, rouge_score['rougeL'].recall, rouge_score['rougeL'].fmeasure\n",
    "    return rouge_p, rouge_r, rouge_f1\n",
    "\n",
    "\n",
    "def bert_score(candidate, reference):\n",
    "    bert_p, bert_r, bert_f1 = bs.score([candidate], [reference])\n",
    "    return bert_p.mean(), bert_r.mean(), bert_f1.mean()\n",
    "\n",
    "\n",
    "def bert_similarity_score(candidate, reference):\n",
    "    inputs1 = bert_tokenizer(reference, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = bert_tokenizer(candidate, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    outputs1 = bert_model(**inputs1)\n",
    "    outputs2 = bert_model(**inputs2)\n",
    "\n",
    "    embeddings1 = outputs1.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    embeddings2 = outputs2.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "    return similarity[0][0]\n",
    "\n",
    "rouge_l_scores = np.zeros((len(cleaned_queries), 3))\n",
    "bert_similarity_scores = np.zeros((len(cleaned_queries), 1))\n",
    "bert_scores = np.zeros((len(cleaned_queries), 3))\n",
    "\n",
    "rouge_l_scores_cleaned = np.zeros((len(cleaned_queries), 3))\n",
    "bert_similarity_scores_cleaned = np.zeros((len(cleaned_queries), 1))\n",
    "bert_scores_cleaned = np.zeros((len(cleaned_queries), 3))\n",
    "\n",
    "for i, (original_query, cleaned_query) in enumerate(cleaned_queries):\n",
    "    _, original_query = original_query\n",
    "    processed_cleaned_query = preprocess_query_final(cleaned_query, stop_words)\n",
    "    \n",
    "    rouge_l_scores[i] = rouge_l_score(cleaned_query, original_query)\n",
    "    bert_similarity_scores[i] = bert_similarity_score(cleaned_query, original_query)\n",
    "    bert_scores[i] = bert_score(cleaned_query, original_query)\n",
    "    \n",
    "    rouge_l_scores_cleaned[i] = rouge_l_score(processed_cleaned_query, original_query)\n",
    "    bert_similarity_scores_cleaned[i] = bert_similarity_score(processed_cleaned_query, original_query)\n",
    "    bert_scores_cleaned[i] = bert_score(processed_cleaned_query, original_query)\n",
    "\n",
    "# Calculate the mean of the rouge_l_scores\n",
    "mean_rouge_l_scores = np.mean(rouge_l_scores, axis=0)\n",
    "mean_bert_similarity_scores = np.mean(bert_similarity_scores)\n",
    "mean_bert_scores = np.mean(bert_scores, axis=0)\n",
    "\n",
    "mean_rouge_l_scores_processed = np.mean(rouge_l_scores_cleaned, axis=0)\n",
    "mean_bert_similarity_scores_processed = np.mean(bert_similarity_scores_cleaned)\n",
    "mean_bert_scores_processed = np.mean(bert_scores_cleaned, axis=0)\n",
    "\n",
    "print(f\"Mean Rouge-L scores: Precision: {mean_rouge_l_scores[0]:.4f}, Recall: {mean_rouge_l_scores[1]:.4f}, F1: {mean_rouge_l_scores[2]:.4f}\")\n",
    "print(f\"Mean BERT Cos Similarity: {mean_bert_similarity_scores:.4f}\")\n",
    "print(f\"Mean Bert-score: Precision: {mean_bert_scores[0]:.4f}, Recall: {mean_bert_scores[1]:.4f}, F1: {mean_bert_scores[2]:.4f}\")\n",
    "\n",
    "print(f\"Mean Rouge-L scores (processed): Precision: {mean_rouge_l_scores_processed[0]:.4f}, Recall: {mean_rouge_l_scores_processed[1]:.4f}, F1: {mean_rouge_l_scores_processed[2]:.4f}\")\n",
    "print(f\"Mean BERT Cos Similarity (processed): {mean_bert_similarity_scores_processed:.4f}\")\n",
    "print(f\"Mean Bert-score (processed): Precision: {mean_bert_scores_processed[0]:.4f}, Recall: {mean_bert_scores_processed[1]:.4f}, F1: {mean_bert_scores_processed[2]:.4f}\")\n",
    "\n",
    "# Save the averages to csv\n",
    "averages = {\n",
    "    'rouge_l_precision': mean_rouge_l_scores[0],\n",
    "    'rouge_l_recall': mean_rouge_l_scores[1],\n",
    "    'rouge_l_f1': mean_rouge_l_scores[2],\n",
    "    'bert_similarity': mean_bert_similarity_scores,\n",
    "    'bert_precision': mean_bert_scores[0],\n",
    "    'bert_recall': mean_bert_scores[1],\n",
    "    'bert_f1': mean_bert_scores[2],\n",
    "    'rouge_l_precision_processed': mean_rouge_l_scores_processed[0],\n",
    "    'rouge_l_recall_processed': mean_rouge_l_scores_processed[1],\n",
    "    'rouge_l_f1_processed': mean_rouge_l_scores_processed[2],\n",
    "    'bert_similarity_processed': mean_bert_similarity_scores_processed,\n",
    "    'bert_precision_processed': mean_bert_scores_processed[0],\n",
    "    'bert_recall_processed': mean_bert_scores_processed[1],\n",
    "    'bert_f1_processed': mean_bert_scores_processed[2]\n",
    "}\n",
    "averages_df = pd.DataFrame(averages, index=[0])\n",
    "averages_df.to_csv('expansion_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2be1b3c5ef3f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:37:29.911289Z",
     "start_time": "2024-03-30T08:37:29.904187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract q_id and original query into separate lists\n",
    "q_ids, original_queries = zip(*[(q_id, query) for (q_id, query), _ in cleaned_queries])\n",
    "rewritten_queries = [rewritten for _, rewritten in cleaned_queries]\n",
    "\n",
    "# Create a DataFrame with separate columns for q_id, original, and rewritten\n",
    "df_queries = pd.DataFrame({\n",
    "    'qid': q_ids,\n",
    "    'original': original_queries,\n",
    "    'rewritten': rewritten_queries\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f74c68a80b18a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:37:29.915657Z",
     "start_time": "2024-03-30T08:37:29.912068Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474214f826c0bede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:37:29.921252Z",
     "start_time": "2024-03-30T08:37:29.916627Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming df_queries and original_queries_df are already defined DataFrames as shown in the images provided\n",
    "# Merge the two DataFrames on 'q_id'\n",
    "merged_df = pd.merge(original_queries_df, df_queries, on='qid', how='left')\n",
    "\n",
    "# Replace NaNs in the 'rewritten' column with the 'original' query from the original_queries_df\n",
    "merged_df['rewritten'] = merged_df['rewritten'].fillna(merged_df['query'])\n",
    "\n",
    "# If you want to rename the 'query' column to 'original' for consistency\n",
    "merged_df = merged_df.rename(columns={'query': 'original_y'})\n",
    "\n",
    "# Now you can drop any redundant columns if they exist (assuming 'original_y' is redundant)\n",
    "merged_df = merged_df.drop(columns=['original', 'original_y'], errors='ignore')\n",
    "\n",
    "# rename the rewritten column to query\n",
    "rewritten_queries_df = merged_df.rename(columns={'rewritten': 'query'})\n",
    "\n",
    "rewritten_queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4427c16e030367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:38:33.453308Z",
     "start_time": "2024-03-30T08:38:33.419983Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use preprocess_query_final on the 'query' column of rewritten_queries_df\n",
    "rewritten_queries_df['query'] = rewritten_queries_df['query'].apply(preprocess_query_final, stop_words=stop_words)\n",
    "\n",
    "rewritten_queries_df[:5].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ccbebb0bc5addd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:38:37.394490Z",
     "start_time": "2024-03-30T08:38:36.495048Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_location = str(Path(\"index\").absolute())\n",
    "index_exists = os.path.isfile(\n",
    "    os.path.join(index_location, \"data.properties\"))\n",
    "\n",
    "# Fetch corpus iterator just before indexing\n",
    "if not index_exists:\n",
    "    corpus_iter = dataset_loader.corpus_iter \n",
    "    indexer = pt.IterDictIndexer(index_location)\n",
    "    index_ref = indexer.index(corpus_iter)\n",
    "    print(\"Indexing completed.\")\n",
    "else:\n",
    "    print(\"Index already exists, loading from disk.\")\n",
    "    index_ref = index_location\n",
    "\n",
    "# Assuming qrels are loaded correctly\n",
    "qrels = dataset_loader.qrels\n",
    "\n",
    "index = pt.IndexFactory.of(index_ref)\n",
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")\n",
    "tf_idf = pt.BatchRetrieve(index, wmodel=\"TF_IDF\")\n",
    "\n",
    "eval_metrics = [\n",
    "    pt.measures.RR(rel=1),\n",
    "    pt.measures.nDCG @ 10,\n",
    "    pt.measures.MAP(rel=1),\n",
    "    pt.measures.Precision @ 5,  # Precision at rank 5\n",
    "    pt.measures.Recall @ 100,   # Recall at rank 100\n",
    "    pt.measures.MRR             # Mean Reciprocal Rank\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42b33d2795bd1a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:38:37.502268Z",
     "start_time": "2024-03-30T08:38:37.500351Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Evaluating Original Queries\n",
    "# print(\"Evaluating Original Queries with BM25 and TF-IDF:\")\n",
    "# results_original = pt.Experiment(\n",
    "#     [bm25, tf_idf],  # List of retrieval systems to evaluate\n",
    "#     original_queries_df[['qid', 'query']],  # DataFrame with queries\n",
    "#     qrels,  # Qrels for relevance judgments\n",
    "#     eval_metrics, \n",
    "#     names=[\"BM25 Original\", \"TF-IDF Original\"]  # Names for the systems\n",
    "# )\n",
    "# \n",
    "# print(f\"Results for Original Queries:\\n{results_original}\")\n",
    "# results_original.to_csv('results_original.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f93451f1692c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:39:07.538937Z",
     "start_time": "2024-03-30T08:38:37.882653Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating Rewritten Queries with BM25 and TF-IDF:\")\n",
    "simple_results = pt.Experiment(\n",
    "    [bm25, tf_idf],\n",
    "    rewritten_queries_df[['qid', 'query']],\n",
    "    qrels,\n",
    "    eval_metrics,\n",
    "    names=[\"BM25 Rewritten\", \"TF-IDF Rewritten\"]\n",
    ")\n",
    "\n",
    "print(f\"Results for Rewritten Queries:\\n{simple_results}\")\n",
    "simple_results.to_csv('rewritten_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542c47fef9ffbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:39:49.119108Z",
     "start_time": "2024-03-30T08:39:49.004950Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the evaluation results\n",
    "results_original = pd.read_csv('results_original.csv')\n",
    "rewritten_results = pd.read_csv('rewritten_results.csv')\n",
    "\n",
    "# Combine the results for easier comparison\n",
    "combined_results = pd.concat([results_original.assign(QueryType='Original'), rewritten_results.assign(QueryType='Rewritten')])\n",
    "\n",
    "# Melt the DataFrame for easier plotting with seaborn\n",
    "melted_results = combined_results.melt(id_vars=['QueryType', 'name'], var_name='Metric', value_name='Score')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Metric', y='Score', hue='name', data=melted_results, ci=None, palette='coolwarm')\n",
    "plt.title('Comparison of IR Models on Original vs. Rewritten Queries')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9837984e5fce05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T08:56:36.556168Z",
     "start_time": "2024-03-30T08:56:36.551724Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Import numpy for handling division by zero\n",
    "\n",
    "# Define a function to calculate percentage improvement\n",
    "def calculate_percentage_improvement(original, rewritten):\n",
    "    # Calculate the improvement for each metric\n",
    "    improvement = (rewritten - original) / original * 100\n",
    "    return improvement\n",
    "\n",
    "# Calculate percentage improvement for each metric\n",
    "percentage_improvement = calculate_percentage_improvement(results_original[['RR', 'nDCG@10', 'AP', 'P@5', 'R@100']],\n",
    "                                                          rewritten_results[['RR', 'nDCG@10', 'AP', 'P@5', 'R@100']])\n",
    "\n",
    "# Create a new DataFrame to store the percentage improvement\n",
    "improvement_df = pd.DataFrame(percentage_improvement, columns=['RR', 'nDCG@10', 'AP', 'P@5', 'R@100'])\n",
    "\n",
    "# Round the values to two decimal places for better readability\n",
    "improvement_df = improvement_df.round(2)\n",
    "\n",
    "# Add a suffix to the column names to indicate improvement compared to original\n",
    "improvement_df.columns = [col + ' Improvement (vs Original)' for col in improvement_df.columns]\n",
    "\n",
    "# Display the nicely formatted DataFrame\n",
    "print(improvement_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
