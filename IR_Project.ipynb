{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "166435ef9e54740c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Install pyterrier it using pip (`pip install python-terrier`).\n",
    "\n",
    "### Step 1: Initialize PyTerrier\n",
    "\n",
    "First, you need to initialize PyTerrier. This is typically done once per notebook or script.\n",
    "\n",
    "```python\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()\n",
    "```\n",
    "\n",
    "### Step 2: Dataset Loading\n",
    "\n",
    "Load your dataset. You can use one of the datasets available in PyTerrier, or load your own dataset.\n",
    "\n",
    "```python\n",
    "# Example of loading a dataset from PyTerrier\n",
    "dataset = pt.get_dataset('irds:your-dataset-name')\n",
    "# For custom datasets, you'd typically load them into a dataframe or similar structure\n",
    "```\n",
    "\n",
    "### Step 3: Preprocessing and Query Rewriting\n",
    "\n",
    "Implement the NLP-based query understanding and rewriting. You might use existing NLP libraries such as spaCy or Hugging Face's Transformers for this purpose.\n",
    "\n",
    "```python\n",
    "from your_query_rewriting_module import rewrite_query\n",
    "\n",
    "# This is a placeholder function. You will implement your query understanding and rewriting logic here.\n",
    "def preprocess_query(query):\n",
    "    # Use NLP techniques to understand and rewrite the query\n",
    "    rewritten_query = rewrite_query(query)\n",
    "    return rewritten_query\n",
    "```\n",
    "\n",
    "### Step 4: Developing the IR System\n",
    "\n",
    "Develop your prototype IR system. This involves setting up the indexing and retrieval process.\n",
    "\n",
    "```python\n",
    "# Indexing\n",
    "indexer = pt.DFIndexer(\"./index_path\", overwrite=True)\n",
    "index_ref = indexer.index(dataset.get_corpus_iter())\n",
    "\n",
    "# Retrieval\n",
    "# You can use one of PyTerrier's built-in retrieval models or develop your own.\n",
    "retriever = pt.BatchRetrieve(index_ref, wmodel=\"BM25\")\n",
    "```\n",
    "\n",
    "### Step 5: Experimentation\n",
    "\n",
    "Run experiments using your IR system. This includes query processing, retrieval, and evaluation against your benchmarks.\n",
    "\n",
    "```python\n",
    "from pyterrier.measures import RR, nDCG\n",
    "\n",
    "# Example query processing and retrieval\n",
    "def run_query_and_evaluate(query):\n",
    "    rewritten_query = preprocess_query(query)\n",
    "    result = retriever.transform(rewritten_query)\n",
    "    # Evaluation - assuming you have a test set with relevance judgments\n",
    "    eval_result = pt.Utils.evaluate(result, qrels, measures=[RR, nDCG])\n",
    "    return eval_result\n",
    "\n",
    "# Example usage\n",
    "query = \"Your test query\"\n",
    "evaluation_metrics = run_query_and_evaluate(query)\n",
    "print(evaluation_metrics)\n",
    "```\n",
    "\n",
    "### Step 6: Analysis\n",
    "\n",
    "Analyze the results to evaluate the effectiveness of your advanced query understanding and rewriting techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f7e502a977965",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Project Setup and Data Preparation\n",
    "\n",
    "## Install PyTerrier\n",
    "\n",
    "Imports the PyTerrier library and initializes it. This step is necessary to use PyTerrier's functionalities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8912d0f4a7342bce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:38:11.338033Z",
     "start_time": "2024-03-19T14:38:10.372487Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-terrier in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (0.10.0)\r\n",
      "Requirement already satisfied: numpy in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.26.4)\r\n",
      "Requirement already satisfied: pandas in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (2.2.0)\r\n",
      "Requirement already satisfied: wget in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (3.2)\r\n",
      "Requirement already satisfied: tqdm in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (4.65.0)\r\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.6.1)\r\n",
      "Requirement already satisfied: matchpy in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.5.5)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.4.0)\r\n",
      "Requirement already satisfied: deprecated in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.2.14)\r\n",
      "Requirement already satisfied: chest in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.2.3)\r\n",
      "Requirement already satisfied: scipy in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.12.0)\r\n",
      "Requirement already satisfied: requests in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (2.31.0)\r\n",
      "Requirement already satisfied: joblib in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.3.2)\r\n",
      "Requirement already satisfied: nptyping==1.4.4 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (1.4.4)\r\n",
      "Requirement already satisfied: more-itertools in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (10.2.0)\r\n",
      "Requirement already satisfied: ir-datasets>=0.3.2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.5.6)\r\n",
      "Requirement already satisfied: jinja2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (3.1.3)\r\n",
      "Requirement already satisfied: statsmodels in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.14.1)\r\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.3.3)\r\n",
      "Requirement already satisfied: dill in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.3.8)\r\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from python-terrier) (0.5.6)\r\n",
      "Requirement already satisfied: typish>=1.7.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\r\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.12.2)\r\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (2.4.0.1)\r\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (5.1.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (6.0.1)\r\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (2.6)\r\n",
      "Requirement already satisfied: lz4>=3.1.10 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (4.3.3)\r\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.3)\r\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.5)\r\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.6)\r\n",
      "Requirement already satisfied: ijson>=3.1.3 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (3.2.3)\r\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.1.12)\r\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-datasets>=0.3.2->python-terrier) (0.2.2)\r\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from requests->python-terrier) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from requests->python-terrier) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from requests->python-terrier) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from requests->python-terrier) (2024.2.2)\r\n",
      "Requirement already satisfied: heapdict in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from chest->python-terrier) (1.0.1)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from deprecated->python-terrier) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from jinja2->python-terrier) (2.1.3)\r\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from matchpy->python-terrier) (2.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from pandas->python-terrier) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from pandas->python-terrier) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from pandas->python-terrier) (2024.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from scikit-learn->python-terrier) (3.3.0)\r\n",
      "Requirement already satisfied: patsy>=0.5.4 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from statsmodels->python-terrier) (0.5.6)\r\n",
      "Requirement already satisfied: packaging>=21.3 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from statsmodels->python-terrier) (23.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from beautifulsoup4>=4.4.1->ir-datasets>=0.3.2->python-terrier) (2.5)\r\n",
      "Requirement already satisfied: six in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from patsy>=0.5.4->statsmodels->python-terrier) (1.16.0)\r\n",
      "Requirement already satisfied: cbor>=1.0.0 in /Users/jasperbruin/anaconda3/envs/IR/lib/python3.10/site-packages (from trec-car-tools>=2.5.4->ir-datasets>=0.3.2->python-terrier) (1.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-terrier\n",
    "\n",
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da56acefc3ca897",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Loads the MS MARCO Passage Ranking dataset for the TREC 2020 Deep Learning Track, which will be used for the information retrieval tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21cbb033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:35:32.395654Z",
     "start_time": "2024-03-19T14:35:32.392936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of loading a dataset from PyTerrier\n",
    "dataset = pt.get_dataset('irds:msmarco-passage/trec-dl-2020')\n",
    "# For custom datasets, you'd typically load them into a dataframe or similar structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d1fe6e279127d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Topics\n",
    "\n",
    "Retrieves and prints the first few search queries (topics) from the dataset. These are the queries that will be used to evaluate the retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03cb029c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:35:32.409273Z",
     "start_time": "2024-03-19T14:35:32.395830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1030303</td>\n",
       "      <td>who is aziz hashim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1037496</td>\n",
       "      <td>who is rep scalise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1043135</td>\n",
       "      <td>who killed nicholas ii of russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1045109</td>\n",
       "      <td>who owns barnhart crane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1049519</td>\n",
       "      <td>who said no one can make you feel inferior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid                                       query\n",
       "0  1030303                          who is aziz hashim\n",
       "1  1037496                          who is rep scalise\n",
       "2  1043135            who killed nicholas ii of russia\n",
       "3  1045109                     who owns barnhart crane\n",
       "4  1049519  who said no one can make you feel inferior"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = dataset.get_topics()\n",
    "topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee930cac79856f78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load Qrels\n",
    "Loads the relevance judgments (qrels) for the dataset, which indicate which documents are relevant to each query. These are used for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "819a40ff2dc7a5e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:35:32.453023Z",
     "start_time": "2024-03-19T14:35:32.407650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "      <th>iteration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23849</td>\n",
       "      <td>1020327</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23849</td>\n",
       "      <td>1034183</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23849</td>\n",
       "      <td>1120730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23849</td>\n",
       "      <td>1139571</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23849</td>\n",
       "      <td>1143724</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid    docno  label iteration\n",
       "0  23849  1020327      2         0\n",
       "1  23849  1034183      3         0\n",
       "2  23849  1120730      0         0\n",
       "3  23849  1139571      1         0\n",
       "4  23849  1143724      0         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = dataset.get_qrels()\n",
    "qrels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d70233721e891a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Access the Corpus\n",
    "\n",
    "Retrieves the first document from the corpus using an iterator. This demonstrates how to access documents in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c3b2002146429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:35:32.454662Z",
     "start_time": "2024-03-19T14:35:32.428232Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] fixing encoding                                                  \n",
      "msmarco-passage/trec-dl-2020 documents:   0%|          | 0/8841823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \n",
      "\u001b[A                                      [INFO] [error] fixing encoding: [00:13] [71.1MB] [5.42MB/s]\n",
      "msmarco-passage/trec-dl-2020 documents:   0%|          | 0/8841823 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m corpus_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(corpus_iter)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m first_doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcorpus_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(first_doc)\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/pyterrier/datasets.py:418\u001b[0m, in \u001b[0;36mIRDSDataset.get_corpus_iter.<locals>.gen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen\u001b[39m():\n\u001b[0;32m--> 418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asdict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyterrier uses \"docno\"\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/util/__init__.py:147\u001b[0m, in \u001b[0;36mDocstoreSplitter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/formats/tsv.py:92\u001b[0m, in \u001b[0;36mTsvIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 92\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mline_iter)\n\u001b[1;32m     93\u001b[0m     cols \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m     num_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\u001b[38;5;241m.\u001b[39m_fields)\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/formats/tsv.py:28\u001b[0m, in \u001b[0;36mFileLineIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctxt\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdlc[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_idx]\u001b[38;5;241m.\u001b[39mstream()))\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctxt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menter_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdlc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart:\n\u001b[1;32m     30\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:505\u001b[0m, in \u001b[0;36m_BaseExitStack.enter_context\u001b[0;34m(self, cm)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object does \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot support the context manager protocol\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 505\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43m_enter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_push_cm_exit(cm, _exit)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/util/fileio.py:78\u001b[0m, in \u001b[0;36mCache.stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/util/fileio.py:69\u001b[0m, in \u001b[0;36mCache.verify\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streamer\u001b[38;5;241m.\u001b[39mstream() \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[0;32m---> 69\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     f\u001b[38;5;241m.\u001b[39mclose() \u001b[38;5;66;03m# close file before move... Needed because of Windows\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mmove(f\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.2_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/shutil.py:197\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m    195\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     buf \u001b[38;5;241m=\u001b[39m fsrc_read(length)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m buf:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/util/fileio.py:35\u001b[0m, in \u001b[0;36mIterStream.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(b):\n\u001b[1;32m     34\u001b[0m     l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(b) \u001b[38;5;241m-\u001b[39m pos  \u001b[38;5;66;03m# We're supposed to return at most this much\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleftover \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit)\n\u001b[1;32m     36\u001b[0m     output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleftover \u001b[38;5;241m=\u001b[39m chunk[:l], chunk[l:]\n\u001b[1;32m     37\u001b[0m     b[pos:pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(output)] \u001b[38;5;241m=\u001b[39m output\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/ir_datasets/datasets/msmarco_passage.py:90\u001b[0m, in \u001b[0;36mFixEncoding.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m pos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(line):\n\u001b[0;32m---> 90\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43mregex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "corpus_iter = dataset.get_corpus_iter()\n",
    "\n",
    "# Convert to an iterator\n",
    "corpus_iterator = iter(corpus_iter)\n",
    "print(1)\n",
    "first_doc = next(corpus_iterator)\n",
    "print(first_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24434cb559f8834",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84185cd77971f62c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Building and Evaluating the Retrieval System\n",
    "\n",
    "## Index the Corpus\n",
    "\n",
    "Indexes the documents in the corpus, preparing them for retrieval. The documents are stored in a specified directory (`./index_path`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "116a4e67e6829bca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:40:42.384828Z",
     "start_time": "2024-03-19T14:40:42.378485Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] fixing encoding                                                  \n",
      "                                                                                   \n",
      "\u001b[A                                      [INFO] [finished] fixing encoding: [09:15] [3.06GB] [5.51MB/s]\n",
      "msmarco-passage/trec-dl-2020 documents:   6%|▌         | 498889/8841823 [10:04<10:27, 13286.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:27:33.395 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Adding an empty document to the index (500080) - further warnings are suppressed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-passage/trec-dl-2020 documents: 100%|██████████| 8841823/8841823 [20:32<00:00, 7176.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:40:53.033 [ForkJoinPool-1-worker-3] WARN org.terrier.structures.indexing.Indexer - Indexed 5 empty documents\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<org.terrier.querying.IndexRef at 0x14c88cb90 jclass=org/terrier/querying/IndexRef jself=<LocalRef obj=0x7f9b06e68428 at 0x15e300fb0>>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = pt.index.IterDictIndexer(\"./index_path\", stemmer=\"porter\", stopwords=\"terrier\")\n",
    "# Index the dataset\n",
    "indexer.index(dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2296cacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "msmarco-passage/trec-dl-2020 documents:   0%|          | 0/8841823 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'The presence of communication amid scientific minds was equally important to the success of the Manhattan Project as scientific intellect was. The only cloud hanging over the impressive achievement of the atomic researchers and engineers is what their success truly meant; hundreds of thousands of innocent lives obliterated.', 'docno': '0'}\n"
     ]
    }
   ],
   "source": [
    "corpus_iter = dataset.get_corpus_iter()\n",
    "\n",
    "# Convert to an iterator\n",
    "corpus_iterator = iter(corpus_iter)\n",
    "\n",
    "first_doc = next(corpus_iterator)\n",
    "print(first_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a05a4400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.8 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from TextBlob) (3.8.1)\n",
      "Requirement already satisfied: click in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from nltk>=3.8->TextBlob) (4.66.2)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.18.0.post0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b88e2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyterrier.transformer import TransformerBase\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "class TransformQuery(TransformerBase):\n",
    "    def __init__(self, index_path):\n",
    "        super().__init__()\n",
    "        self.stopwords_set = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.index_path = index_path\n",
    "\n",
    "    def _preprocess(self, query):\n",
    "        # Lowercase the query\n",
    "        query = query.lower()\n",
    "        # Remove punctuation\n",
    "        query = query.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Tokenize the query\n",
    "        tokens = query.split()\n",
    "        # Remove stopwords and apply stemming\n",
    "        filtered_tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stopwords_set]\n",
    "        # Join the filtered tokens back into a query\n",
    "        return ' '.join(filtered_tokens)\n",
    "\n",
    "    def _expand_query(self, query):\n",
    "        # Use TextBlob for basic synonym expansion\n",
    "        blob = TextBlob(query)\n",
    "        expanded_query = blob.words.lemmatize()\n",
    "        return \" \".join(expanded_query)\n",
    "\n",
    "    def transform(self, query):\n",
    "        # Preprocess the query\n",
    "        preprocessed_query = self._preprocess(query)\n",
    "        # Expand the query\n",
    "        expanded_query = self._expand_query(preprocessed_query)\n",
    "        return expanded_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5f07557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/zoe/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a213fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         qid                                       query\n",
      "0    1030303                          who is aziz hashim\n",
      "1    1037496                          who is rep scalise\n",
      "2    1043135            who killed nicholas ii of russia\n",
      "3    1045109                     who owns barnhart crane\n",
      "4    1049519  who said no one can make you feel inferior\n",
      "..       ...                                         ...\n",
      "195   985594                          where is kampuchea\n",
      "196    99005                 convert sq meter to sq inch\n",
      "197   997622          where is the show shameless filmed\n",
      "198   999466                            where is velbert\n",
      "199   132622               definition of attempted arson\n",
      "\n",
      "[200 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "queries = dataset.get_topics()\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a17d33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query \t\t 0: who is aziz hashim\n",
      "Transformed Query \t 0: aziz hashim\n",
      "\n",
      "Original Query \t\t 1: who is rep scalise\n",
      "Transformed Query \t 1: rep scalis\n",
      "\n",
      "Original Query \t\t 2: who killed nicholas ii of russia\n",
      "Transformed Query \t 2: kill nichola ii russia\n",
      "\n",
      "Original Query \t\t 3: who owns barnhart crane\n",
      "Transformed Query \t 3: own barnhart crane\n",
      "\n",
      "Original Query \t\t 4: who said no one can make you feel inferior\n",
      "Transformed Query \t 4: said one make feel inferior\n",
      "\n",
      "Original Query \t\t 5: who sings monk theme song\n",
      "Transformed Query \t 5: sing monk theme song\n",
      "\n",
      "Original Query \t\t 6: who was the highest career passer rating in the nfl\n",
      "Transformed Query \t 6: highest career passer rate nfl\n",
      "\n",
      "Original Query \t\t 7: why do hunters pattern their shotguns\n",
      "Transformed Query \t 7: hunter pattern shotgun\n",
      "\n",
      "Original Query \t\t 8: why do some places on my scalp feel sore\n",
      "Transformed Query \t 8: place scalp feel sore\n",
      "\n",
      "Original Query \t\t 9: why is pete rose banned from hall of fame\n",
      "Transformed Query \t 9: pete rose ban hall fame\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/f489rvfn4hsdfx4c91m6h1b40000gn/T/ipykernel_22847/531188234.py:9: DeprecationWarning: Call to deprecated method __init__. (Use pt.Transformer instead of TransformerBase) -- Deprecated since version 0.9.\n",
      "  super().__init__()\n"
     ]
    }
   ],
   "source": [
    "index_path = \"./index_path\"\n",
    "queries = dataset.get_topics()\n",
    "\n",
    "# Instantiate TransformQuery with the index path\n",
    "transformer = TransformQuery(index_path=\"./index_path\")\n",
    "\n",
    "# Apply transformation to the first 10 queries\n",
    "for query_id, query_row in queries.head(10).iterrows():\n",
    "    # Extract the query text from the DataFrame row\n",
    "    query_text = query_row['query']  # Assuming the column name is 'query'\n",
    "\n",
    "    # Apply transformation\n",
    "    transformed_query = transformer.transform(query_text)\n",
    "    print(f\"Original Query \\t\\t {query_id}: {query_text}\")\n",
    "    print(f\"Transformed Query \\t {query_id}: {transformed_query}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73548a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-terrier in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (0.10.0)\n",
      "Requirement already satisfied: transformers in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (4.39.0)\n",
      "Requirement already satisfied: numpy in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.26.2)\n",
      "Requirement already satisfied: pandas in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (2.1.3)\n",
      "Requirement already satisfied: wget in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: tqdm in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (4.66.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: matchpy in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.4.1.post1)\n",
      "Requirement already satisfied: deprecated in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: chest in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: scipy in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.12.0)\n",
      "Requirement already satisfied: requests in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (2.31.0)\n",
      "Requirement already satisfied: joblib in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: more_itertools in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (10.2.0)\n",
      "Requirement already satisfied: ir_datasets>=0.3.2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: jinja2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (3.1.3)\n",
      "Requirement already satisfied: statsmodels in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.14.1)\n",
      "Requirement already satisfied: ir_measures>=0.3.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: dill in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.3.8)\n",
      "Requirement already satisfied: pytrec_eval_terrier>=0.5.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: typish>=1.7.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: filelock in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (4.12.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (2.4.0.1)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (5.1.0)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (4.3.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (0.2.3)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (0.1.6)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (3.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (0.1.12)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_datasets>=0.3.2->python-terrier) (0.2.2)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from ir_measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from requests->python-terrier) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from requests->python-terrier) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from requests->python-terrier) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from requests->python-terrier) (2024.2.2)\n",
      "Requirement already satisfied: heapdict in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from jinja2->python-terrier) (2.1.5)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from pandas->python-terrier) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from pandas->python-terrier) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from pandas->python-terrier) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from scikit-learn->python-terrier) (3.3.0)\n",
      "Requirement already satisfied: patsy>=0.5.4 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from statsmodels->python-terrier) (0.5.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from beautifulsoup4>=4.4.1->ir_datasets>=0.3.2->python-terrier) (2.5)\n",
      "Requirement already satisfied: six in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from patsy>=0.5.4->statsmodels->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages (from trec-car-tools>=2.5.4->ir_datasets>=0.3.2->python-terrier) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-terrier transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7a8cf509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers', 'hparams_name', 'hyper_parameters'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MODEL_PATH = \"./models/doc_baseline.ckpt\"\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device('cpu'))\n",
    "\n",
    "print(state_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ce635e40",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertModel:\n\tMissing key(s) in state_dict: \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classification.weight\", \"classification.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[101], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel(config)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the state dictionary into the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Ensure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BertModel:\n\tMissing key(s) in state_dict: \"embeddings.word_embeddings.weight\", \"embeddings.position_embeddings.weight\", \"embeddings.token_type_embeddings.weight\", \"embeddings.LayerNorm.weight\", \"embeddings.LayerNorm.bias\", \"encoder.layer.0.attention.self.query.weight\", \"encoder.layer.0.attention.self.query.bias\", \"encoder.layer.0.attention.self.key.weight\", \"encoder.layer.0.attention.self.key.bias\", \"encoder.layer.0.attention.self.value.weight\", \"encoder.layer.0.attention.self.value.bias\", \"encoder.layer.0.attention.output.dense.weight\", \"encoder.layer.0.attention.output.dense.bias\", \"encoder.layer.0.attention.output.LayerNorm.weight\", \"encoder.layer.0.attention.output.LayerNorm.bias\", \"encoder.layer.0.intermediate.dense.weight\", \"encoder.layer.0.intermediate.dense.bias\", \"encoder.layer.0.output.dense.weight\", \"encoder.layer.0.output.dense.bias\", \"encoder.layer.0.output.LayerNorm.weight\", \"encoder.layer.0.output.LayerNorm.bias\", \"encoder.layer.1.attention.self.query.weight\", \"encoder.layer.1.attention.self.query.bias\", \"encoder.layer.1.attention.self.key.weight\", \"encoder.layer.1.attention.self.key.bias\", \"encoder.layer.1.attention.self.value.weight\", \"encoder.layer.1.attention.self.value.bias\", \"encoder.layer.1.attention.output.dense.weight\", \"encoder.layer.1.attention.output.dense.bias\", \"encoder.layer.1.attention.output.LayerNorm.weight\", \"encoder.layer.1.attention.output.LayerNorm.bias\", \"encoder.layer.1.intermediate.dense.weight\", \"encoder.layer.1.intermediate.dense.bias\", \"encoder.layer.1.output.dense.weight\", \"encoder.layer.1.output.dense.bias\", \"encoder.layer.1.output.LayerNorm.weight\", \"encoder.layer.1.output.LayerNorm.bias\", \"encoder.layer.2.attention.self.query.weight\", \"encoder.layer.2.attention.self.query.bias\", \"encoder.layer.2.attention.self.key.weight\", \"encoder.layer.2.attention.self.key.bias\", \"encoder.layer.2.attention.self.value.weight\", \"encoder.layer.2.attention.self.value.bias\", \"encoder.layer.2.attention.output.dense.weight\", \"encoder.layer.2.attention.output.dense.bias\", \"encoder.layer.2.attention.output.LayerNorm.weight\", \"encoder.layer.2.attention.output.LayerNorm.bias\", \"encoder.layer.2.intermediate.dense.weight\", \"encoder.layer.2.intermediate.dense.bias\", \"encoder.layer.2.output.dense.weight\", \"encoder.layer.2.output.dense.bias\", \"encoder.layer.2.output.LayerNorm.weight\", \"encoder.layer.2.output.LayerNorm.bias\", \"encoder.layer.3.attention.self.query.weight\", \"encoder.layer.3.attention.self.query.bias\", \"encoder.layer.3.attention.self.key.weight\", \"encoder.layer.3.attention.self.key.bias\", \"encoder.layer.3.attention.self.value.weight\", \"encoder.layer.3.attention.self.value.bias\", \"encoder.layer.3.attention.output.dense.weight\", \"encoder.layer.3.attention.output.dense.bias\", \"encoder.layer.3.attention.output.LayerNorm.weight\", \"encoder.layer.3.attention.output.LayerNorm.bias\", \"encoder.layer.3.intermediate.dense.weight\", \"encoder.layer.3.intermediate.dense.bias\", \"encoder.layer.3.output.dense.weight\", \"encoder.layer.3.output.dense.bias\", \"encoder.layer.3.output.LayerNorm.weight\", \"encoder.layer.3.output.LayerNorm.bias\", \"encoder.layer.4.attention.self.query.weight\", \"encoder.layer.4.attention.self.query.bias\", \"encoder.layer.4.attention.self.key.weight\", \"encoder.layer.4.attention.self.key.bias\", \"encoder.layer.4.attention.self.value.weight\", \"encoder.layer.4.attention.self.value.bias\", \"encoder.layer.4.attention.output.dense.weight\", \"encoder.layer.4.attention.output.dense.bias\", \"encoder.layer.4.attention.output.LayerNorm.weight\", \"encoder.layer.4.attention.output.LayerNorm.bias\", \"encoder.layer.4.intermediate.dense.weight\", \"encoder.layer.4.intermediate.dense.bias\", \"encoder.layer.4.output.dense.weight\", \"encoder.layer.4.output.dense.bias\", \"encoder.layer.4.output.LayerNorm.weight\", \"encoder.layer.4.output.LayerNorm.bias\", \"encoder.layer.5.attention.self.query.weight\", \"encoder.layer.5.attention.self.query.bias\", \"encoder.layer.5.attention.self.key.weight\", \"encoder.layer.5.attention.self.key.bias\", \"encoder.layer.5.attention.self.value.weight\", \"encoder.layer.5.attention.self.value.bias\", \"encoder.layer.5.attention.output.dense.weight\", \"encoder.layer.5.attention.output.dense.bias\", \"encoder.layer.5.attention.output.LayerNorm.weight\", \"encoder.layer.5.attention.output.LayerNorm.bias\", \"encoder.layer.5.intermediate.dense.weight\", \"encoder.layer.5.intermediate.dense.bias\", \"encoder.layer.5.output.dense.weight\", \"encoder.layer.5.output.dense.bias\", \"encoder.layer.5.output.LayerNorm.weight\", \"encoder.layer.5.output.LayerNorm.bias\", \"encoder.layer.6.attention.self.query.weight\", \"encoder.layer.6.attention.self.query.bias\", \"encoder.layer.6.attention.self.key.weight\", \"encoder.layer.6.attention.self.key.bias\", \"encoder.layer.6.attention.self.value.weight\", \"encoder.layer.6.attention.self.value.bias\", \"encoder.layer.6.attention.output.dense.weight\", \"encoder.layer.6.attention.output.dense.bias\", \"encoder.layer.6.attention.output.LayerNorm.weight\", \"encoder.layer.6.attention.output.LayerNorm.bias\", \"encoder.layer.6.intermediate.dense.weight\", \"encoder.layer.6.intermediate.dense.bias\", \"encoder.layer.6.output.dense.weight\", \"encoder.layer.6.output.dense.bias\", \"encoder.layer.6.output.LayerNorm.weight\", \"encoder.layer.6.output.LayerNorm.bias\", \"encoder.layer.7.attention.self.query.weight\", \"encoder.layer.7.attention.self.query.bias\", \"encoder.layer.7.attention.self.key.weight\", \"encoder.layer.7.attention.self.key.bias\", \"encoder.layer.7.attention.self.value.weight\", \"encoder.layer.7.attention.self.value.bias\", \"encoder.layer.7.attention.output.dense.weight\", \"encoder.layer.7.attention.output.dense.bias\", \"encoder.layer.7.attention.output.LayerNorm.weight\", \"encoder.layer.7.attention.output.LayerNorm.bias\", \"encoder.layer.7.intermediate.dense.weight\", \"encoder.layer.7.intermediate.dense.bias\", \"encoder.layer.7.output.dense.weight\", \"encoder.layer.7.output.dense.bias\", \"encoder.layer.7.output.LayerNorm.weight\", \"encoder.layer.7.output.LayerNorm.bias\", \"encoder.layer.8.attention.self.query.weight\", \"encoder.layer.8.attention.self.query.bias\", \"encoder.layer.8.attention.self.key.weight\", \"encoder.layer.8.attention.self.key.bias\", \"encoder.layer.8.attention.self.value.weight\", \"encoder.layer.8.attention.self.value.bias\", \"encoder.layer.8.attention.output.dense.weight\", \"encoder.layer.8.attention.output.dense.bias\", \"encoder.layer.8.attention.output.LayerNorm.weight\", \"encoder.layer.8.attention.output.LayerNorm.bias\", \"encoder.layer.8.intermediate.dense.weight\", \"encoder.layer.8.intermediate.dense.bias\", \"encoder.layer.8.output.dense.weight\", \"encoder.layer.8.output.dense.bias\", \"encoder.layer.8.output.LayerNorm.weight\", \"encoder.layer.8.output.LayerNorm.bias\", \"encoder.layer.9.attention.self.query.weight\", \"encoder.layer.9.attention.self.query.bias\", \"encoder.layer.9.attention.self.key.weight\", \"encoder.layer.9.attention.self.key.bias\", \"encoder.layer.9.attention.self.value.weight\", \"encoder.layer.9.attention.self.value.bias\", \"encoder.layer.9.attention.output.dense.weight\", \"encoder.layer.9.attention.output.dense.bias\", \"encoder.layer.9.attention.output.LayerNorm.weight\", \"encoder.layer.9.attention.output.LayerNorm.bias\", \"encoder.layer.9.intermediate.dense.weight\", \"encoder.layer.9.intermediate.dense.bias\", \"encoder.layer.9.output.dense.weight\", \"encoder.layer.9.output.dense.bias\", \"encoder.layer.9.output.LayerNorm.weight\", \"encoder.layer.9.output.LayerNorm.bias\", \"encoder.layer.10.attention.self.query.weight\", \"encoder.layer.10.attention.self.query.bias\", \"encoder.layer.10.attention.self.key.weight\", \"encoder.layer.10.attention.self.key.bias\", \"encoder.layer.10.attention.self.value.weight\", \"encoder.layer.10.attention.self.value.bias\", \"encoder.layer.10.attention.output.dense.weight\", \"encoder.layer.10.attention.output.dense.bias\", \"encoder.layer.10.attention.output.LayerNorm.weight\", \"encoder.layer.10.attention.output.LayerNorm.bias\", \"encoder.layer.10.intermediate.dense.weight\", \"encoder.layer.10.intermediate.dense.bias\", \"encoder.layer.10.output.dense.weight\", \"encoder.layer.10.output.dense.bias\", \"encoder.layer.10.output.LayerNorm.weight\", \"encoder.layer.10.output.LayerNorm.bias\", \"encoder.layer.11.attention.self.query.weight\", \"encoder.layer.11.attention.self.query.bias\", \"encoder.layer.11.attention.self.key.weight\", \"encoder.layer.11.attention.self.key.bias\", \"encoder.layer.11.attention.self.value.weight\", \"encoder.layer.11.attention.self.value.bias\", \"encoder.layer.11.attention.output.dense.weight\", \"encoder.layer.11.attention.output.dense.bias\", \"encoder.layer.11.attention.output.LayerNorm.weight\", \"encoder.layer.11.attention.output.LayerNorm.bias\", \"encoder.layer.11.intermediate.dense.weight\", \"encoder.layer.11.intermediate.dense.bias\", \"encoder.layer.11.output.dense.weight\", \"encoder.layer.11.output.dense.bias\", \"encoder.layer.11.output.LayerNorm.weight\", \"encoder.layer.11.output.LayerNorm.bias\", \"pooler.dense.weight\", \"pooler.dense.bias\". \n\tUnexpected key(s) in state_dict: \"bert.embeddings.position_ids\", \"bert.embeddings.word_embeddings.weight\", \"bert.embeddings.position_embeddings.weight\", \"bert.embeddings.token_type_embeddings.weight\", \"bert.embeddings.LayerNorm.weight\", \"bert.embeddings.LayerNorm.bias\", \"bert.encoder.layer.0.attention.self.query.weight\", \"bert.encoder.layer.0.attention.self.query.bias\", \"bert.encoder.layer.0.attention.self.key.weight\", \"bert.encoder.layer.0.attention.self.key.bias\", \"bert.encoder.layer.0.attention.self.value.weight\", \"bert.encoder.layer.0.attention.self.value.bias\", \"bert.encoder.layer.0.attention.output.dense.weight\", \"bert.encoder.layer.0.attention.output.dense.bias\", \"bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.encoder.layer.0.intermediate.dense.weight\", \"bert.encoder.layer.0.intermediate.dense.bias\", \"bert.encoder.layer.0.output.dense.weight\", \"bert.encoder.layer.0.output.dense.bias\", \"bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.encoder.layer.1.attention.self.query.weight\", \"bert.encoder.layer.1.attention.self.query.bias\", \"bert.encoder.layer.1.attention.self.key.weight\", \"bert.encoder.layer.1.attention.self.key.bias\", \"bert.encoder.layer.1.attention.self.value.weight\", \"bert.encoder.layer.1.attention.self.value.bias\", \"bert.encoder.layer.1.attention.output.dense.weight\", \"bert.encoder.layer.1.attention.output.dense.bias\", \"bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.encoder.layer.1.intermediate.dense.weight\", \"bert.encoder.layer.1.intermediate.dense.bias\", \"bert.encoder.layer.1.output.dense.weight\", \"bert.encoder.layer.1.output.dense.bias\", \"bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.encoder.layer.2.attention.self.query.weight\", \"bert.encoder.layer.2.attention.self.query.bias\", \"bert.encoder.layer.2.attention.self.key.weight\", \"bert.encoder.layer.2.attention.self.key.bias\", \"bert.encoder.layer.2.attention.self.value.weight\", \"bert.encoder.layer.2.attention.self.value.bias\", \"bert.encoder.layer.2.attention.output.dense.weight\", \"bert.encoder.layer.2.attention.output.dense.bias\", \"bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.encoder.layer.2.intermediate.dense.weight\", \"bert.encoder.layer.2.intermediate.dense.bias\", \"bert.encoder.layer.2.output.dense.weight\", \"bert.encoder.layer.2.output.dense.bias\", \"bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.encoder.layer.3.attention.self.query.weight\", \"bert.encoder.layer.3.attention.self.query.bias\", \"bert.encoder.layer.3.attention.self.key.weight\", \"bert.encoder.layer.3.attention.self.key.bias\", \"bert.encoder.layer.3.attention.self.value.weight\", \"bert.encoder.layer.3.attention.self.value.bias\", \"bert.encoder.layer.3.attention.output.dense.weight\", \"bert.encoder.layer.3.attention.output.dense.bias\", \"bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.encoder.layer.3.intermediate.dense.weight\", \"bert.encoder.layer.3.intermediate.dense.bias\", \"bert.encoder.layer.3.output.dense.weight\", \"bert.encoder.layer.3.output.dense.bias\", \"bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.encoder.layer.4.attention.self.query.weight\", \"bert.encoder.layer.4.attention.self.query.bias\", \"bert.encoder.layer.4.attention.self.key.weight\", \"bert.encoder.layer.4.attention.self.key.bias\", \"bert.encoder.layer.4.attention.self.value.weight\", \"bert.encoder.layer.4.attention.self.value.bias\", \"bert.encoder.layer.4.attention.output.dense.weight\", \"bert.encoder.layer.4.attention.output.dense.bias\", \"bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.encoder.layer.4.intermediate.dense.weight\", \"bert.encoder.layer.4.intermediate.dense.bias\", \"bert.encoder.layer.4.output.dense.weight\", \"bert.encoder.layer.4.output.dense.bias\", \"bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.encoder.layer.5.attention.self.query.weight\", \"bert.encoder.layer.5.attention.self.query.bias\", \"bert.encoder.layer.5.attention.self.key.weight\", \"bert.encoder.layer.5.attention.self.key.bias\", \"bert.encoder.layer.5.attention.self.value.weight\", \"bert.encoder.layer.5.attention.self.value.bias\", \"bert.encoder.layer.5.attention.output.dense.weight\", \"bert.encoder.layer.5.attention.output.dense.bias\", \"bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.encoder.layer.5.intermediate.dense.weight\", \"bert.encoder.layer.5.intermediate.dense.bias\", \"bert.encoder.layer.5.output.dense.weight\", \"bert.encoder.layer.5.output.dense.bias\", \"bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.encoder.layer.6.attention.self.query.weight\", \"bert.encoder.layer.6.attention.self.query.bias\", \"bert.encoder.layer.6.attention.self.key.weight\", \"bert.encoder.layer.6.attention.self.key.bias\", \"bert.encoder.layer.6.attention.self.value.weight\", \"bert.encoder.layer.6.attention.self.value.bias\", \"bert.encoder.layer.6.attention.output.dense.weight\", \"bert.encoder.layer.6.attention.output.dense.bias\", \"bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.encoder.layer.6.intermediate.dense.weight\", \"bert.encoder.layer.6.intermediate.dense.bias\", \"bert.encoder.layer.6.output.dense.weight\", \"bert.encoder.layer.6.output.dense.bias\", \"bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.encoder.layer.7.attention.self.query.weight\", \"bert.encoder.layer.7.attention.self.query.bias\", \"bert.encoder.layer.7.attention.self.key.weight\", \"bert.encoder.layer.7.attention.self.key.bias\", \"bert.encoder.layer.7.attention.self.value.weight\", \"bert.encoder.layer.7.attention.self.value.bias\", \"bert.encoder.layer.7.attention.output.dense.weight\", \"bert.encoder.layer.7.attention.output.dense.bias\", \"bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.encoder.layer.7.intermediate.dense.weight\", \"bert.encoder.layer.7.intermediate.dense.bias\", \"bert.encoder.layer.7.output.dense.weight\", \"bert.encoder.layer.7.output.dense.bias\", \"bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.encoder.layer.8.attention.self.query.weight\", \"bert.encoder.layer.8.attention.self.query.bias\", \"bert.encoder.layer.8.attention.self.key.weight\", \"bert.encoder.layer.8.attention.self.key.bias\", \"bert.encoder.layer.8.attention.self.value.weight\", \"bert.encoder.layer.8.attention.self.value.bias\", \"bert.encoder.layer.8.attention.output.dense.weight\", \"bert.encoder.layer.8.attention.output.dense.bias\", \"bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.encoder.layer.8.intermediate.dense.weight\", \"bert.encoder.layer.8.intermediate.dense.bias\", \"bert.encoder.layer.8.output.dense.weight\", \"bert.encoder.layer.8.output.dense.bias\", \"bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.encoder.layer.9.attention.self.query.weight\", \"bert.encoder.layer.9.attention.self.query.bias\", \"bert.encoder.layer.9.attention.self.key.weight\", \"bert.encoder.layer.9.attention.self.key.bias\", \"bert.encoder.layer.9.attention.self.value.weight\", \"bert.encoder.layer.9.attention.self.value.bias\", \"bert.encoder.layer.9.attention.output.dense.weight\", \"bert.encoder.layer.9.attention.output.dense.bias\", \"bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.encoder.layer.9.intermediate.dense.weight\", \"bert.encoder.layer.9.intermediate.dense.bias\", \"bert.encoder.layer.9.output.dense.weight\", \"bert.encoder.layer.9.output.dense.bias\", \"bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.encoder.layer.10.attention.self.query.weight\", \"bert.encoder.layer.10.attention.self.query.bias\", \"bert.encoder.layer.10.attention.self.key.weight\", \"bert.encoder.layer.10.attention.self.key.bias\", \"bert.encoder.layer.10.attention.self.value.weight\", \"bert.encoder.layer.10.attention.self.value.bias\", \"bert.encoder.layer.10.attention.output.dense.weight\", \"bert.encoder.layer.10.attention.output.dense.bias\", \"bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.encoder.layer.10.intermediate.dense.weight\", \"bert.encoder.layer.10.intermediate.dense.bias\", \"bert.encoder.layer.10.output.dense.weight\", \"bert.encoder.layer.10.output.dense.bias\", \"bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.encoder.layer.11.attention.self.query.weight\", \"bert.encoder.layer.11.attention.self.query.bias\", \"bert.encoder.layer.11.attention.self.key.weight\", \"bert.encoder.layer.11.attention.self.key.bias\", \"bert.encoder.layer.11.attention.self.value.weight\", \"bert.encoder.layer.11.attention.self.value.bias\", \"bert.encoder.layer.11.attention.output.dense.weight\", \"bert.encoder.layer.11.attention.output.dense.bias\", \"bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.encoder.layer.11.intermediate.dense.weight\", \"bert.encoder.layer.11.intermediate.dense.bias\", \"bert.encoder.layer.11.output.dense.weight\", \"bert.encoder.layer.11.output.dense.bias\", \"bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.pooler.dense.weight\", \"bert.pooler.dense.bias\", \"classification.weight\", \"classification.bias\". "
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "import torch\n",
    "\n",
    "MODEL_PATH = \"./models/doc_baseline.ckpt\"\n",
    "state_dict = torch.load(MODEL_PATH, map_location=torch.device('cpu'))[\"state_dict\"]\n",
    "\n",
    "# Create a default configuration for the BERT model\n",
    "config = BertConfig()\n",
    "\n",
    "# Initialize the BERT model\n",
    "model = BertModel(config)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Ensure token embedding weights are still tied if needed\n",
    "model.tie_weights()\n",
    "\n",
    "# Set model in evaluation mode to deactivate Dropout modules by default\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "061ecfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f9/f489rvfn4hsdfx4c91m6h1b40000gn/T/ipykernel_22847/2140005613.py:6: DeprecationWarning: Call to deprecated method __init__. (Use pt.Transformer instead of TransformerBase) -- Deprecated since version 0.9.\n",
      "  super().__init__()\n",
      "/Users/zoe/Desktop/test-datasets/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1985: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the fine-tuned BERT model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/zoe/Downloads/doc_query2doc.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 21\u001b[0m query_expander \u001b[38;5;241m=\u001b[39m \u001b[43mFineTunedBERTQueryExpander\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Use the query expander in a PyTerrier pipeline\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyterrier\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n",
      "Cell \u001b[0;32mIn[61], line 7\u001b[0m, in \u001b[0;36mFineTunedBERTQueryExpander.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2086\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2083\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2084\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2097\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2325\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   2328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2329\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2330\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/transformers/models/bert/tokenization_bert.py:204\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocab_file):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    203\u001b[0m     )\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab \u001b[38;5;241m=\u001b[39m \u001b[43mload_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mids_to_tokens \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict([(ids, tok) \u001b[38;5;28;01mfor\u001b[39;00m tok, ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize \u001b[38;5;241m=\u001b[39m do_basic_tokenize\n",
      "File \u001b[0;32m~/Desktop/test-datasets/venv/lib/python3.11/site-packages/transformers/models/bert/tokenization_bert.py:121\u001b[0m, in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    119\u001b[0m vocab \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mOrderedDict()\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m reader:\n\u001b[0;32m--> 121\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadlines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tokens):\n\u001b[1;32m    123\u001b[0m     token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from pyterrier.transformer import TransformerBase\n",
    "\n",
    "class FineTunedBERTQueryExpander(TransformerBase):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "        self.model = BertModel.from_pretrained(model_path)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    def transform(self, query):\n",
    "        inputs = self.tokenizer(query, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        # Process the outputs to obtain the expanded query\n",
    "        expanded_query = ...  # Implement your query expansion logic here\n",
    "        return expanded_query\n",
    "\n",
    "# Load the fine-tuned BERT model\n",
    "model_path = \"/Users/zoe/Downloads/doc_query2doc.ckpt\"\n",
    "query_expander = FineTunedBERTQueryExpander(model_path)\n",
    "\n",
    "# Use the query expander in a PyTerrier pipeline\n",
    "from pyterrier.pipeline import Pipeline\n",
    "from pyterrier.transformer import TransformerBase\n",
    "from pyterrier.datasets import get_dataset\n",
    "\n",
    "\n",
    "pipe = Pipeline()\n",
    "pipe.append(query_expander)\n",
    "pipe.fit(topics)\n",
    "\n",
    "# Use the pipeline to expand queries\n",
    "expanded_queries = pipe.transform(topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca1c9088481f10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Configure Retrieval Model\n",
    "Configures the retrieval model to use the BM25 weighting model for ranking documents in response to queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "871842303b1a7363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:43:14.351131Z",
     "start_time": "2024-03-19T14:43:13.642443Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure the retrieval model\n",
    "bm25 = pt.BatchRetrieve(indexer, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "937b187139015250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:43:15.000652Z",
     "start_time": "2024-03-19T14:43:14.352126Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = pt.BatchRetrieve(indexer, wmodel=\"TF_IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf3204b01ada1a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO: Run queries and evaluate the retrieval system \n",
    "\n",
    "We can now run an experiment that evaluates both models on the same data. For better readability, we assign custom names to both approaches.\n",
    "Comparing the approaches in the same experiment allows us to automatically have statistical significance testing performed. By setting baseline=0, we tell the function to compute the  $p$-values with respect to the first approach (TF-IDF). Furthermore, PyTerrier supports a number of correction methods; here, we apply Bonferroni correction:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "170adc2fe68cba38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T14:46:46.926348Z",
     "start_time": "2024-03-19T14:46:20.420561Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>RR@10</th>\n",
       "      <th>nDCG@20</th>\n",
       "      <th>AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.802102</td>\n",
       "      <td>0.479753</td>\n",
       "      <td>0.358072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.802102</td>\n",
       "      <td>0.479866</td>\n",
       "      <td>0.358724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name     RR@10   nDCG@20        AP\n",
       "0  TF-IDF  0.802102  0.479753  0.358072\n",
       "1    BM25  0.802102  0.479866  0.358724"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyterrier.measures import RR, nDCG, MAP\n",
    "from pathlib import Path\n",
    "\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "pt.Experiment(\n",
    "    [tf_idf, bm25],\n",
    "    dataset.get_topics(),\n",
    "    dataset.get_qrels(),\n",
    "    names=[\"TF-IDF\", \"BM25\"],\n",
    "    eval_metrics=[RR @ 10, nDCG @ 20, MAP],\n",
    "    save_dir=str(results_dir),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613310c7c960a04",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
